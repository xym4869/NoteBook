---

typora-root-url: Typora图库\DeepLearning
---

[TOC]
# 第一章  机器学习基础

## 1.1 基本概念
**大数据**通常被定义为“超出常用软件工具捕获，管理和处理能力”的数据集。 
**机器学习**关心的问题是如何构建计算机程序使用经验自动改进。
**数据挖掘**是从数据中提取模式的特定算法的应用，在数据挖掘中，重点在于算法的应用，而不是算法本身。
**机器学习和数据挖掘**之间的关系：
- 数据挖掘是一个过程，在此过程中机器学习算法被用作提取数据集中的潜在有价值模式的工具。
- 深度学习是一种模拟大脑的行为。可以从所学习对象的机制以及行为等等很多相关联的方面进行学习，模仿类型行为以及思维。
- 深度学习对于大数据的发展有帮助。深度学习对于大数据技术开发的每一个阶段均有帮助，不管是数据的分析还是挖掘还是建模，只有深度学习，这些工作才会有可能一一得到实现。
- 深度学习对于大数据的发展有帮助。深度学习对于大数据技术开发的每一个阶段均有帮助，不管是数据的分析还是挖掘还是建模，只有深度学习，这些工作才会有可能一一得到实现。
- 深度学习转变了解决问题的思维。在深度学习的基础上，要求我们从开始到最后都要基于一个目标，为了需要优化的那个最终目标去进行处理数据以及将数据放入到数据应用平台上去，这就是端到端（End to End）。
- 大数据的深度学习需要一个框架。在大数据方面的深度学习都是从基础的角度出发的，深度学习需要一个框架或者一个系统。总而言之，将你的大数据通过深度分析变为现实，这就是深度学习和大数据的最直接关系。

**机器学习**：对于一个任务及其表现的度量方法，设计一种算法，让算法能够提取中数据所蕴含的规律
**神经网络**：按照一定规则将多个神经元连接起来的网络。
![1.1.1](/1.1.1.png)

### 1.1.1 计算图的导数计算
计算图导数计算是反向传播，利用链式法则和隐式函数求导。
假设 $z = f(u,v)$ 在点 $(u,v)$ 处偏导连续，$(u,v)$是关于 $t$ 的函数，在 $t$ 点可导，求 $z$ 在 $t$ 点的导数。
$$
\frac{dz}{dt}=\frac{\partial z}{\partial u}.\frac{du}{dt}+\frac{\partial z}{\partial v}
				.\frac{dv}{dt}
$$
### 1.1.2 局部最优与全局最优

- 局部最优，就是在函数值空间的一个有限区域内寻找最小值；而全局最优，是在函数值空间整个区域寻找最小值问题。
- 函数局部最小点是它的函数值小于或等于附近点的点，但是有可能大于较远距离的点。
- 全局最小点是那种它的函数值小于或等于所有的可行点。

## 1.2 机器学习学习方式
### 1.2.1 监督学习：输入机器的数据是带有标签的
- 监督学习是使用已知正确答案的示例来训练网络。已知数据和其一一对应的标签，训练一个预测模型，将输入数据映射到标签的过程。
- 常见应用场景：分类问题和回归问题。
- 算法举例：
	- 	支持向量机(Support Vector Machine, SVM)
	- 	朴素贝叶斯(Naive Bayes)
	- 	逻辑回归(Logistic Regression)
	- 	K近邻(K-Nearest Neighborhood, KNN)
	- 	决策树(Decision Tree)
	- 	随机森林(Random Forest)
	- 	AdaBoost
	- 	线性判别分析(Linear Discriminant Analysis, LDA)
	- 	深度学习(Deep Learning)也是大多数以监督学习的方式呈现。

### 1.2.2 非监督学习：具有数据集但无标签
- 在非监督式学习中，数据并不被特别标识，适用于你具有数据集但无标签的情况。学习模型是为了推断出数据的一些**内在结构**。
- 常见应用场景：关联规则的学习以及聚类等。
- 算法举例：
	- Apriori算法
	- k-Means算法
### 1.2.3 半监督学习：输入数据部分被标记，部分没有被标记
- 输入数据部分被标记，部分没有被标记，这种学习模型可以用来进行预测。
- 常见应用场景：包括分类和回归，算法包括一些对常用监督式学习算法的延伸，通过对已标记数据建模，在此基础上，对未标记数据进行预测。
- 算法举例：
	- 图论推理算法（Graph Inference）
	- 拉普拉斯支持向量机（Laplacian SVM）
### 1.2.4 弱监督学习：数据集的标签是不可靠的
- 监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或包含多种情况（没有标记，有一个标记，和有多个标记）的多个元素。** 数据集的标签是不可靠的**，这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。
- <u>已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。</u>标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签。
- 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。
### 1.2.5 监督学习有哪些步骤
1. 数据集的创建和分类
2. 数据增强（Data Augmentation）
	- 当原始数据搜集和标注完毕，一般搜集的数据并不一定包含目标在各种扰动下的信息。数据的好坏对于机器学习模型的预测能力至关重要，因此一般会进行数据增强。
	- <u>对于图像数据来说，数据增强一般包括，图像旋转，平移，颜色变换，裁剪，仿射变换等。</u>
3. 特征工程（Feature Engineering）：包含特征提取和特征选择。
	- 常见的手工特征(Hand-Crafted Feature)有尺度不变特征变换(Scale-Invariant Feature Transform, SIFT)，方向梯度直方图(Histogram of Oriented Gradient, HOG)等
	- 由于手工特征是启发式的，其算法设计背后的出发点不同，将这些特征组合在一起的时候有可能会产生冲突，**如何将组合特征的效能发挥出来，使原始数据在特征空间中的判别性最大化**，就需要用到**特征选择**的方法
	- 最常用到的卷积神经网络(Convolutional Neural Networks, CNNs)本身就是一种特征提取和选择的引擎。
	- 研究者提出的不同的网络结构、正则化、归一化方法实际上就是深度学习背景下的特征工程。
4. 构建预测模型和损失
	- 如何保证模型的输出和输入标签的一致性，就需要构建模型预测和标签之间的损失函数
	- 常见的损失函数(Loss Function)有交叉熵、均方差等
	- 通过优化方法不断迭代，使模型从最初的初始化状态一步步变化为有预测能力的模型的过程，实际上就是学习的过程。
5. 训练
	- 选择合适的模型和超参数进行初始化
	- 超参数比如支持向量机中核函数、误差项惩罚权重等
	- 当模型初始化参数设定好后，将制作好的特征数据输入到模型，通过合适的优化方法不断缩小输出与标签之间的差距，当迭代过程到了截止条件，就可以得到训练好的模型。
	- 优化方法最常见的就是梯度下降法及其变种，使用梯度下降法的前提是优化目标函数对于模型是可导的。
6. 验证和模型选择
	- 通常会通过调整和模型相关的各种事物（超参数）来重复步骤2和3
7. 测试及应用
## 1.3 分类算法

分类算法和回归算法是对真实世界不同建模的方法。
- 分类模型是认为模型的输出是离散的，例如大自然的生物被划分为不同的种类，是离散的。
- 回归模型的输出是连续的，例如人的身高变化过程是一个连续过程，而不是离散的。
因此，在实际建模过程时，采用分类模型还是回归模型，取决于你对任务（真实世界）的分析和理解。
### 1.3.1 常用分类算法的优缺点
|算法|优点|缺点|
|:-|:-|:-|
|Bayes 贝叶斯分类法|1）所需估计的参数少，对于缺失数据不敏感。<br />2）有着坚实的数学基础，以及稳定的分类效率。|1）需要假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。<br />2）需要知道先验概率。<br />3）分类决策存在错误率。|
|Decision Tree决策树|1）不需要任何领域知识或参数假设。<br />2）适合高维数据。<br />3）简单易于理解。<br />4）短时间内处理大量数据，得到可行且效果较好的结果。<br />5）能够同时处理数据型和常规性属性。|1）对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。<br />2）易于过拟合。<br />3）忽略属性之间的相关性。<br />4）不支持在线学习。|
|SVM支持向量机|1）可以解决小样本下机器学习的问题。<br />2）提高泛化性能。<br />3）可以解决高维、非线性问题。超高维文本分类仍受欢迎。<br />4）避免神经网络结构选择和局部极小的问题。|1）对缺失数据敏感。<br />2）内存消耗大，难以解释。<br />3）运行和调参略烦人。|
|KNN K近邻|1）思想简单，理论成熟，既可以用来做分类也可以用来做回归； <br />2）可用于非线性分类；<br /> 3）训练时间复杂度为O(n)； <br />4）准确度高，对数据没有假设，对outlier不敏感；|1）计算量太大。<br />2）对于样本分类不均衡的问题，会产生误判。<br />3）需要大量的内存。<br />4）输出的可解释性不强。|
|Logistic Regression逻辑回归|1）速度快。<br />2）简单易于理解，直接看到各个特征的权重。<br />3）能容易地更新模型吸收新的数据。<br />4）如果想要一个概率框架，动态调整分类阀值。|特征处理复杂。需要归一化和较多的特征工程。|
|Neural Network 神经网络|1）分类准确率高。<br />2）并行处理能力强。<br />3）分布式存储和学习能力强。<br />4）鲁棒性较强，不易受噪声影响。|1）需要大量参数（网络拓扑、阀值、阈值）。<br />2）结果难以解释。<br />3）训练时间过长。|
|Adaboosting|1）adaboost是一种有很高精度的分类器。<br />2）可以使用各种方法构建子分类器，Adaboost算法提供的是框架。<br />3）当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。<br />4）简单，不用做特征筛选。<br />5）不用担心overfitting。|对outlier比较敏感|

### 1.3.2 正确率能很好的评估分类算法吗

不能。
正确率确实是一个很直观很好的评价指标，但是有时候正确率高并不能完全代表一个算法就好。比如对某个地区进行地震预测，地震分类属性分为0：不发生地震、1发生地震。我们都知道，不发生的概率是极大的，对于分类器而言，如果分类器不加思考，对每一个测试样例的类别都划分为0，达到99%的正确率，但是，问题来了，如果真的发生地震时，这个分类器毫无察觉，那带来的后果将是巨大的。很显然，99%正确率的分类器并不是我们想要的。出现这种现象的原因主要是**数据分布不均衡，类别为1的数据太少，错分了类别1但达到了很高的正确率**却忽视了研究者本身最为关注的情况。

### 1.3.3 什么样的分类器是最好的

某个具体的分类器不可能同时满足或提高所有上面介绍的指标。
如果一个分类器能正确分对所有的实例，那么各项指标都已经达到最优，但这样的分类器往往不存在。比如之前说的地震预测，既然不能百分百预测地震的发生，但实际情况中能容忍一定程度的误报。假设在1000次预测中，共有5次预测发生了地震，真实情况中有一次发生了地震，其他4次则为误报。正确率由原来的999/1000=99.9下降为996/1000=99.6。召回率由0/1=0%上升为1/1=100%。对此解释为，虽然预测失误了4次，但真的地震发生前，分类器能预测对，没有错过，这样的分类器实际意义更为重大，正是我们想要的。**在这种情况下，在一定正确率前提下，要求分类器的召回率尽量高。**

### 1.3.4 分类和回归的区别

1. 分类问题中，输出不仅仅只允许取两个值，可以允许多个值，它是离散的；而在回归问题中，输出可取任意实数，是连续的。 
2.  分类和回归的区别在于输出变量的类型。  定量输出称为回归，或者说是连续变量预测；  定性输出称为分类，或者说是离散变量预测。 
3.  分类基本上都是用“回归模型”解决的，只是假设的模型不同(损失函数不一样)，因为不能把分类标签当回归问题的输出来解决。 
4.  回归问题通常是用来预测一个值 ； 分类问题是用于*将事物打上一个标签，通常结果为离散值*。  分类通常是建立在回归之上，分类的最后一层通常要使用softmax函数进行判断其所属类别。分类并没有逼近的概念，最终正确结果只有一个，错误的就是错误的，不会有相近的概念。 

## 1.4 模型评估
### 1.4.1 常用方法
**分类模型常用评估方法：**

|指标|描述|
|:-:|:-:|
|Accuracy|准确率|
|Precision|精准度/查准率|
|Recall|召回率/查全率|
|P-R曲线|查准率为纵轴，查全率为横轴，作图|
|F1|F1值|
|Confusion Matrix|混淆矩阵|
|ROC|ROC曲线|
|AUC|ROC曲线下的面积|

**回归模型常用评估方法：**

|指标|描述|
|:-:|:-:|
|Mean Square Error (MSE, RMSE)|平均方差|
|Absolute Error (MAE, RAE)|绝对误差|
|R-Squared|R平方值|

### 1.4.2 误差、偏差和方差
在机器学习中，Bias(偏差)，Error(误差)，和Variance(方差)存在以下区别和联系：
**对于Error **：
- 误差（error）：一般地，我们把**学习器的实际预测输出与样本的真实输出之间的差异**称为“误差”。
- Error = Bias + Variance + Noise，**Error反映的是整个模型的准确度**。

**对于Noise:**
噪声：描述了在当前任务上任何学习算法所能达到的**期望泛化误差的下界**，即**刻画了学习问题本身的难度**。

**对于Bias：**
- Bias衡量**模型拟合训练数据的能力**（训练数据不一定是整个 training dataset，而是只用于训练它的那一部分数据，例如：mini-batch），Bias反映的是**模型在样本上的输出与真实值之间的误差，即==模型本身的精准度==**。
- Bias 越小，拟合能力越高（可能产生overfitting）；反之，拟合能力越低（可能产生underfitting）。
- **偏差越大，越偏离真实数据**，如下图第二行所示。

**对于Variance：**
- 方差公式：$S_{N}^{2}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}$
- Variance描述的是**预测值的变化范围，离散程度**，也就是离其期望值的距离。方差越大，数据的分布越分散，模型的稳定程度越差。
- Variance反映的是**模型每一次输出结果与模型输出期望之间的误差，即==模型的稳定性==**。
- Variance越小，模型的泛化的能力越高；反之，模型的泛化的能力越低。
- 如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差劣，则方差较大，说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。 如下图右列所示。

![1.4.2](/1.4.2.png)

 ### 1.4.3 经验误差与泛化误差
 经验误差（empirical error）：也叫训练误差（training error），模型在训练集上的误差。 
 泛化误差（generalization error）：模型在新样本集（测试集）上的误差称为“泛化误差”。

 ### 1.4.4 欠拟合、过拟合
​	模型欠拟合：在训练集以及测试集上同时具有较高的误差，此时**模型的偏差较大**；
​	模型过拟合：在训练集上具有较低的误差，在测试集上具有较高的误差，此时**模型的方差较大**。
​	模型正常：在训练集以及测试集上，同时具有相对较低的偏差以及方差。

### 1.4.5 如何解决过拟合与欠拟合
**如何解决欠拟合：**

1. 添加其他特征项。组合、泛化、相关性、上下文特征、平台特征等特征是特征添加的重要手段，有时候特征项不够会导致模型欠拟合。
2. 添加多项式特征。例如将线性模型添加二次项或三次项使模型泛化能力更强。例如，FM（Factorization Machine）模型、FFM（Field-aware Factorization Machine）模型，其实就是线性模型，增加了二阶多项式，保证了模型一定的拟合程度。
3. 可以增加模型的复杂程度。
4. 减小正则化系数。正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。

**如何解决过拟合：**

1. 重新清洗数据，数据不纯会导致过拟合，此类情况需要重新清洗数据。 
2. 增加训练样本数量。 
3. 降低模型复杂程度。 
4. 增大正则项系数。 
5. 采用dropout方法，dropout方法，通俗的讲就是在训练的时候让神经元以一定的概率不工作。 
6. early stopping。 
7. 减少迭代次数。 
8. 增大学习率。 
9. 添加噪声数据。 
10. 树结构中，可以对树进行剪枝。 
11. 减少特征项。

欠拟合和过拟合这些方法，需要根据实际问题，实际模型，进行选择。

### 1.4.6 交叉验证的主要作用
为了得到更为稳健可靠的模型，对模型的泛化误差进行评估，得到模型泛化误差的近似值。当有多个模型可以选择时，我们通常选择“泛化误差”最小的模型。 
交叉验证的方法有许多种，但是最常用的是：留一交叉验证、k折交叉验证。

### 1.4.7 理解k折交叉验证
1. 将含有N个样本的数据集，分成K份，每份含有N/K个样本。选择其中1份作为测试集，另外K-1份作为训练集，测试集就有K种情况。 
2. 在每种情况中，用训练集训练模型，用测试集测试模型，计算模型的泛化误差。 
3. 交叉验证重复K次，每份验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测，得到模型最终的泛化误差。 
4. 将K种情况下，模型的泛化误差取均值，得到模型最终的泛化误差。  
5. 一般$2\leqslant K \leqslant10$。 k折交叉验证的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。 
6. 训练集中样本数量要足够多，一般至少大于总样本数的50%。 
7. 训练集和测试集必须从完整的数据集中均匀取样。均匀取样的目的是希望减少训练集、测试集与原数据集之间的偏差。当样本数量足够多时，通过随机取样，便可以实现均匀取样的效果。 

### 1.4.8 混淆矩阵
第一种混淆矩阵:

|真实情况T or F|预测为正例1，P|预测为负例0，N|
|:-:|:-|:-|
|本来label标记为1，预测结果真为T、假为F|TP(预测为1，实际为1)|FN(预测为0，实际为1)|
|本来label标记为0，预测结果真为T、假为F|FP(预测为1，实际为0)|TN(预测为0，实际也为0)|

第二种混淆矩阵:

|预测情况P or N|实际label为1,预测对了为T|实际label为0,预测对了为T|
|:-:|:-|:-|
|预测为正例1，P|TP(预测为1，实际为1)|FP(预测为1，实际为0)|
|预测为负例0，N|FN(预测为0，实际为1)|TN(预测为0，实际也为0)|

### 1.4.9 评估方法
现在假设我们的分类目标只有两类，计为正例（positive）和负例（negative）分别是：
   1) True positives(TP):  被正确地划分为正例的个数，即实际为正例且被分类器划分为正例的实例数；
   2) False positives(FP): 被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的实例数；
   3) False negatives(FN):被错误地划分为负例的个数，即实际为正例但被分类器划分为负例的实例数；
   4) True negatives(TN): 被正确地划分为负例的个数，即实际为负例且被分类器划分为负例的实例数。

![1.3.2](/1.4.9.png)
说明：
	1）P=TP+FN表示实际为正例的样本个数。
	2）True、False描述的是分类器是否判断正确。
	3）Positive、Negative是分类器的分类结果，如果正例计为1、负例计为-1，即positive=1、negative=-1。用1表示True，-1表示False
	4）例如True positives(TP)的实际类标=1\*1=1为正例，False positives(FP)的实际类标=(-1)\*1=-1为负例，False negatives(FN)的实际类标=(-1)\*(-1)=1为正例，True negatives(TN)的实际类标=1\*(-1)=-1为负例。

**评价指标**
  1) 正确率（accuracy）：被分对的样本数在所有样本数中的占比
  	accuracy = (TP+TN)/(P+N)。通常来说，正确率越高，分类器越好。
  2) 错误率（error rate)：描述被分类器错分的比例，分类错误的样本数占样本总数的比例。
  	error rate = (FP+FN)/(P+N) = 1 - error rate
  3) 灵敏度（sensitivity）：所有正例中被分对的比例，衡量了分类器对正例的识别能力
  	sensitivity = TP/P
  4) 特异性（specificity)：所有负例中被分对的比例，衡量了分类器对负例的识别能力。
  	specificity = TN/N
  5) 精度（precision）：被分为正例的示例中实际为正例的比例。（分类正确的样本数占样本总数的比例。）
  	precision=TP/(TP+FP)
  6) 召回率（recall）：同灵敏度，是覆盖面的度量，度量有多个正例被分为正例
  7)   其他评价指标
  	计算速度：分类器训练和预测需要的时间；
  	鲁棒性：处理缺失值和异常值的能力；
  	可扩展性：处理大数据集的能力；
  	可解释性：分类器的预测标准的可理解性，像决策树产生的规则就是很容易理解的，而神经网络的一堆参数就不好理解，我们只好把它看成一个黑盒子。
  8)**精度和召回率反映了分类器分类性能的两个方面**。
  ==查准率（精度）预测出为阳性的样本中，正确的有多少。==区别准确率（正确预测出的样本，包括正确预测为阳性、阴性，占总样本比例）。
  例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。
  ==查全率（召回率）正确预测为阳性的数量占总样本中阳性数量的比例。==
  例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 
  9)如果综合考虑查准率与查全率，可以得到新的评价指标F1-score，也称为**综合分类率**：
$$
F1=\frac{2 \times precision \times recall}{precision + recall}
$$
	- 宏平均F1的计算方法先对每个类别单独计算F1值，再取这些F1值的算术平均值作为全局指标。
	- 微平均F1的计算方法是先累加计算各个类别的a、b、c、d的值，再由这些值求出F1值。
	- 宏平均F1平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均F1平等考虑文档集中的每一个文档，所以它的值受到常见类别的影响比较大。

### 1.4.10 ROC曲线

ROC曲线是（Receiver Operating Characteristic Curve，受试者工作特征曲线）的简称，通过将连续变量设定出多个不同的临界值，从而计算出一系列真正率和假正率，再以假正率为横坐标、真正率为纵坐标绘制成曲线，曲线下面积越大，推断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为假正率和真正率均较高的临界值。 ROC曲线的横坐标为False Positive Rate（FPR，1减去特异性（假阳性率）），纵坐标为True Positive Rate（TPR，灵敏度（真阳性率））。
![1.4.10](/1.4.10.png)

- 可以将不同模型对同一数据集的ROC曲线绘制在同一笛卡尔坐标系中，ROC曲线越靠近左上角，说明其对应模型越可靠
- 可以通过ROC曲线下面的面积（Area Under Curve, AUC）来评价模型，**AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）**。AUC越大，模型越可靠。

下面着重介绍ROC曲线图中的四个点和一条线。
	第一个点(0,1)，即FPR=0, TPR=1，这意味着FN（False Negative）=0，并且FP（False Positive）=0。意味着这是一个完美的分类器，它将所有的样本都正确分类。
	第二个点(1,0)，即FPR=1，TPR=0，意味着这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。
	第三个点(0,0)，即FPR=TPR=0，即FP（False Positive）=TP（True Positive）=0，可以发现该分类器预测所有的样本都为负样本（Negative）。
	第四个点(1,1)，即FPR=TPR=1，分类器实际上预测所有的样本都为正样本。
经过以上分析，ROC曲线越接近左上角，该分类器的性能越好。

关于ROC的绘制和计算，参考第一章引用部分：如何画ROC曲线，如何计算TPR,FPR。

### 1.4.11 AUC
**如何计算AUC**
- 将坐标点按照横坐标FPR排序 。
- 计算第$i$个坐标点和第$i+1$个坐标点的间距$dx$ 。 
- 获取第$i$或者$i+1$个坐标点的纵坐标y。
- 计算面积微元$ds=ydx$。
- 对面积微元进行累加，得到AUC。

​	下图展现了三种AUC的值： 

![](/1.4.11.1.png)

​	AUC是衡量二分类模型优劣的一种评价指标，表示正例排在负例前面的概率。其他评价指标有精确度、准确率、召回率，而AUC比这三者更为常用。
​	一般在分类模型中，预测结果都是以概率的形式表现，如果要计算准确率，通常都会手动设置一个阈值来将对应的概率转化成类别，这个阈值也就很大程度上影响了模型准确率的计算。

关于AUC的计算，参考第一章引用部分：AUC计算画图。	

### 1.4.12 为什么使用Roc和Auc评价分类器
模型有很多评估方法，为什么还要使用ROC和AUC呢？
因为ROC曲线有个很好的特性：**当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变**。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。

### 1.4.13 PR曲线
PR曲线是Precision Recall Curve的简称，描述的是precision和recall之间的关系，以recall为横坐标，precision为纵坐标绘制的曲线。

- 该曲线的所对应的面积AUC实际上是目标检测中常用的评价指标平均精度（Average Precision, AP）。AP越高，说明模型性能越好。

### 1.4.14 代价敏感错误率与代价曲线
不同的错误会产生不同代价。以二分法为例，设置代价矩阵如下：

![1.4.14](/1.4.14.png)

当判断正确的时候，值为0，不正确的时候，分别为$Cost_{01}$和$Cost_{10}$ 。
$Cost_{10}$:表示实际为反例但预测成正例的代价。
$Cost_{01}$:表示实际为正例但是预测为反例的代价。

**代价敏感错误率**=样本中由模型得到的错误值与代价乘积之和 / 总样本。
其数学表达式为：
$$
E(f;D;cost)=\frac{1}{m}\left( \sum_{x_{i} \in D^{+}}({f(x_i)\neq y_i})\times Cost_{01}+ \sum_{x_{i} \in D^{-}}({f(x_i)\neq y_i})\times Cost_{10}\right)
$$
$D^{+}、D^{-}$分别代表样例集的正例子集和反例子集，x是预测值，y是真实值。

**代价曲线**：
在均等代价时，ROC曲线不能直接反应出模型的期望总体代价，而代价曲线可以。
代价曲线横轴为[0,1]的正例函数代价：
$$
P(+)Cost=\frac{p*Cost_{01}}{p*Cost_{01}+(1-p)*Cost_{10}}
$$
其中p是样本为正例的概率。
代价曲线纵轴维[0,1]的归一化代价：
$$
Cost_{norm}=\frac{FNR*p*Cost_{01}+FNR*(1-p)*Cost_{10}}{p*Cost_{01}+(1-p)*Cost_{10}}
$$
其中FPR为假阳率，FNR=1-TPR为假阴率。

注：ROC每个点，对应代价平面上一条线。
例如，ROC上(TPR,FPR),计算出FNR=1-TPR，在代价平面上绘制一条从(0,FPR)到(1,FNR)的线段，面积则为该条件下期望的总体代价。所有线段下界面积，所有条件下学习器的期望总体代价。

![1.4.14.1](/1.4.14.1.png)

### 1.4.15 模型有哪些比较检验方法
正确性分析：模型稳定性分析，稳健性分析，收敛性分析，变化趋势分析，极值分析等。
有效性分析：误差分析，参数敏感性分析，模型对比检验等。
有用性分析：关键数据求解，极值点，拐点，变化趋势分析，用数据验证动态模拟等。
高效性分析：时空复杂度分析与现有进行比较等。

### 1.4.16 为什么使用标准差

方差公式为：$S^2_{N}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}$
标准差公式为：$S_{N}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}}$
样本标准差公式为：$S_{N}=\sqrt{\frac{1}{N-1}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}}$

与方差相比，使用标准差来表示数据点的离散程度有3个好处：
1、表示离散程度的数字与样本数据点的**数量级一致**，更适合对数据样本形成感性认知。
2、表示离散程度的数字单位与样本数据的**单位一致**，更方便做后续的分析运算。
3、在样本数据**大致符合正态分布**的情况下，标准差具有方便估算的特性：68%的数据点落在平均值前后1个标准差的范围内、95%的数据点落在平均值前后2个标准差的范围内，而99%的数据点将会落在平均值前后3个标准差的范围内。

### 1.4.17 类别不平衡产生原因
类别不平衡（class-imbalance）是指分类任务中不同类别的训练样例数目差别很大的情况。 
产生原因：
分类学习算法通常都会假设不同类别的训练样例数目基本相同。如果不同类别的训练样例数目差别很大，则会影响学习结果，测试结果变差。例如二分类问题中有998个反例，正例有2个，那学习方法只需返回一个永远将新样本预测为反例的分类器，就能达到99.8%的精度；然而这样的分类器没有价值。

### 1.4.18 常见的类别不平衡问题解决方法
防止类别不平衡对学习造成的影响，在构建分类模型之前，需要对分类不平衡性问题进行处理。主要解决方法有：
1、扩大数据集
​	增加包含小类样本数据的数据，更多的数据能得到更多的分布信息。

2、对大类数据欠采样
​	减少大类数据样本个数，使与小样本个数接近。
​	缺点：欠采样操作时若随机丢弃大类样本，可能会丢失重要信息。 
​	代表算法：EasyEnsemble。其思想是利用集成学习机制，将大类划分为若干个集合供不同的学习器使用。相当于对每个学习器都进行欠采样，但对于全局则不会丢失重要信息。

3、对小类数据过采样
​	过采样：对小类的数据样本进行采样来增加小类的数据样本个数。 
​	代表算法：SMOTE和ADASYN。 
​	SMOTE：通过对训练集中的小类数据进行插值来产生额外的小类样本数据。
​	新的少数类样本产生的策略：对每个少数类样本a，在a的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为新合成的少数类样本。 	
​	ADASYN：根据学习难度的不同，对不同的少数类别的样本使用加权分布，对于难以学习的少数类的样本，产生更多的综合数据。 通过减少类不平衡引入的偏差和将分类决策边界自适应地转移到困难的样本两种手段，改善了数据分布。

4、使用新评价指标
​	如果当前评价指标不适用，则应寻找其他具有说服力的评价指标。比如准确度这个评价指标在类别不均衡的分类任务中并不适用，甚至进行误导。因此在类别不均衡分类任务中，需要使用更有说服力的评价指标来对分类器进行评价。

5、选择新算法
​	不同的算法适用于不同的任务与数据，应该使用不同的算法进行比较。

6、数据代价加权
​	例如当分类任务是识别小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值，从而使得分类器将重点集中在小类样本身上。

7、转化问题思考角度
​	例如在分类问题时，把小类的样本作为异常点，将问题转化为异常点检测或变化趋势检测问题。 异常点检测即是对那些罕见事件进行识别。变化趋势检测区别于异常点检测在于其通过检测不寻常的变化趋势来识别。	

8、将问题细化分析
​	对问题进行分析与挖掘，将问题划分成多个更小的问题，看这些小问题是否更容易解决。 

## 1.5 回归算法

### 1.5.1 回归分类

（1）如果是连续的，就是多重线性回归。
（2）如果是二项分布，就是逻辑回归。
（3）如果是泊松（Poisson）分布，就是泊松回归。
（4）如果是负二项分布，就是负二项回归。
（5）逻辑回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以**实际中最常用的就是二分类的逻辑回归**。

### 1.5.2 线性回归
线性回归问题：试图学到一个线性模型尽可能准确地预测新样本的输出值
- 对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；
- 对于离散值的属性，可作下面的处理：
	- 若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。
	- 若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。

线性回归试图学习：
$$
f(x_i)=wx_i+b,使得f(x_i) \approx y_i
$$
如何确定$w$和$b$呢？我们可以试图让均方误差（MSE，欧式距离）最小化
$$
(w^*,b^*) = \underset{(w,b)}{arg min}\sum_{i=1}^m{(f(x_i)-y_i)^2}=\underset{(w,b)}{arg min}\sum_{i=1}^m{(y_i-wx_i-b)^2}
$$
基于均方误差最小化来进行模型求解的方法称为**“最小二乘法”（Euclidean distance）**
求解$w$和$b$使$E_{(w,b)}=\sum_{i=1}^m{(y_i-wx_i-b)^2}$最小化的过程，称为线性回归模型的最小二乘“参数估计”。
我们将$E_{(w,b)}$分别对$w$和$b$求导，得到
$$
\frac{\partial E_{(w,b)}}{\partial w} = 2\bigg(w\sum_{i=1}^mx^2_i-\sum_{i=1}^m(y_i-b)x_i\bigg) = 0, \\ 
\frac{\partial E_{(w,b)}}{\partial w} = 2\bigg(mb-\sum_{i=1}^m(y_i-wx_i)\bigg) = 0
$$
得到$w$和$b$最优解的闭式解
$$
w=\frac{\sum_{i=1}^my_i(x_i-\overline x)}{\sum_{i=1}^mx_i^2-\frac{1}{m}(\sum_{i=1}^mx_i)^2}, \\
b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)
$$
更一般的情况是样本由d个属性描述，即**“多元线性回归”（multivariate linear regression）**
$$
f(\boldsymbol x_i)=\boldsymbol w^T\boldsymbol x_i+b, 使得f(\boldsymbol x_i)\approx y_i
$$
类似的，可利用最小二乘法对$\boldsymbol w$和$b$进行估计。
$$
\hat{\boldsymbol w}^*=\underset{\hat{\boldsymbol w}}{arg min}(\boldsymbol y-\boldsymbol X\hat{\boldsymbol w})^T(\boldsymbol y-\boldsymbol X\hat{\boldsymbol w})
$$
我们把$\boldsymbol w$和$b$吸收入向量形式$\hat{\boldsymbol w}=(\boldsymbol w;b)$，令$E_ {\boldsymbol{\hat w}}=(\boldsymbol y-\boldsymbol X\hat{\boldsymbol w})^T(\boldsymbol y-\boldsymbol X\hat{\boldsymbol w})$，对$\hat{\boldsymbol w}$求导，得
$$
\frac{\partial E_ {\boldsymbol{\hat w}}}{\partial \hat{\boldsymbol w}}=2\boldsymbol X^T(\boldsymbol X\hat{\boldsymbol w}-\boldsymbol y)
$$
令上式为零可得$\hat{\boldsymbol w}$最优解的闭式解。
当$\boldsymbol X^T\boldsymbol X$为满秩矩阵或正定矩阵时，可得
$$
\hat{\boldsymbol w}^*=(\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol y
$$
最终学得的线性回归模型为
$$
f(\hat{\boldsymbol x}_i)=\hat{\boldsymbol x}_i^T(\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol y
$$
然而现实任务中$\boldsymbol X^T\boldsymbol X$往往不是满秩矩阵，会解出多个$\hat{w}$均能使均方误差最小化。选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入**正则化项**。

若y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，实际上就是相当于将指数曲线投影在一条直线上：
$$
ln y =\boldsymbol w^T\boldsymbol x+b
$$
更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。
$$
y=g^{-1}(\boldsymbol w^T\boldsymbol x+b)
$$
### 1.5.3 逻辑回归（Logistic Regression）
使用一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。**对数几率函数（Sigmoid函数）**正是这样的一个常用的替代函数：
$$
y=\frac{1}{1+e^{-z}}
$$
 ![1.5.3](/1.5.3.png)
带入$g^{-1}(·)$中，得到
$$
y=\frac{1}{1+e^{-(\boldsymbol w^T\boldsymbol x+b)}}
$$
可变化为
$$
ln\frac{y}{1-y}=\boldsymbol w^T\boldsymbol x+b
$$
若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为**“对数几率回归”（logistic regression）**，又称为**“逻辑回归”**
### 1.5.4 逻辑回归为什么使用对数损失函数
逻辑回归它假设样本服从伯努利分布（0-1分布），进而求得满足该分布的似然函数，接着取对数求极值等。逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看，就是对数损失函数。
假设逻辑回归模型
$$
P(y=1|x;\theta)=\frac{1}{1+e^{-\theta^{T}x}}
$$
假设逻辑回归模型的概率分布是伯努利分布，其概率质量函数为：
$$
P(X=n)=
\begin{cases}
1-p, n=0\\
 p,n=1
\end{cases}
$$
其似然函数为：
$$
L(\theta)=\prod_{i=1}^{m}
P(y=1|x_i)^{y_i}P(y=0|x_i)^{1-y_i}
$$
对数似然函数为：
$$
\ln L(\theta)=\sum_{i=1}^{m}[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln{P(y=0|x_i)}]\\
  =\sum_{i=1}^m[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln(1-P(y=1|x_i))]
$$
对数函数在单个数据点上的定义为：（取负是满足非负，log函数在[0,1]区间为负。让**最大似然值**与**最小损失**对应起来）
$$
cost(y,p(y|x))=-y\ln{p(y|x)-(1-y)\ln(1-p(y|x))}
$$
则全局样本损失函数为：
$$
cost(y,p(y|x)) = -\sum_{i=1}^m[y_i\ln p(y_i|x_i)+(1-y_i)\ln(1-p(y_i|x_i))]
$$
由此可看出，**对数损失函数与极大似然估计的对数似然函数本质上是相同的**。所以逻辑回归直接采用对数损失函数。
> 最大似然损失函数：常用在分类问题上。形式上是把每一个预测值的概率相乘，得到一个损失值。

###  1.5.5 逻辑回归适用性
（1）用于概率预测。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。
（2）用于分类。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。**进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。**
（3）寻找危险因素。寻找某一疾病的危险因素等。
（4）**仅能用于线性问题**。只有当目标和特征是线性关系时，才能用逻辑回归。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。
（5）各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。
### 1.5.6 线性回归与逻辑回归的区别
（1）线性回归的样本的输出，都是连续值，$ y\in (-\infty ,+\infty )$，而逻辑回归中$y\in (0,1)$，只能取0和1。
（2）对于拟合函数也有本质上的差别： 
	- 线性回归：$f(x)=\theta ^{T}x=\theta _{1}x _{1}+\theta _{2}x _{2}+...+\theta _{n}x _{n}$
	- 逻辑回归：$f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)$，其中，$g(z)=\frac{1}{1+e^{-z}}$
	可以看出，**线性回归的拟合函数，是对f(x)的输出变量y的拟合，而逻辑回归的拟合函数是对为1类样本的概率的拟合**。
	那么，为什么要以1类样本的概率进行拟合呢，为什么可以这样拟合呢？ 
	1. $\theta ^{T}x=0$就相当于是1类和0类的决策边界： 
	2.  当$\theta ^{T}x>0$，则y>0.5；若$\theta ^{T}x\rightarrow +\infty $，则$y \rightarrow  1 $，即y为1类; 
	3. 当$\theta ^{T}x<0$，则y<0.5；若$\theta ^{T}x\rightarrow -\infty $，则$y \rightarrow  0 $，即y为0类; 
	这个时候就能看出区别，**在线性回归中$\theta ^{T}x$为预测值的拟合函数；而在逻辑回归中$\theta ^{T}x$为决策边界**。

线性回归和逻辑回归的区别：
|         | 线性回归         | 逻辑回归  |
|:-------------:|:-------------:|:-----:|
| 目的     | 预测 |分类 |
|  $y^{(i)}$   | 未知     |   （0,1）|
| 函数 | 拟合函数     |   预测函数 |
| 参数计算方式| 最小二乘法      |    极大似然估计 |

下面具体解释一下： 
1. 拟合函数和预测函数什么关系呢？简单来说就是将拟合函数做了一个逻辑函数的转换，转换后使得$y^{(i)} \in (0,1)$;
2. 最小二乘和最大似然估计可以相互替代吗？回答当然是不行了。我们来看看两者依仗的原理：**最大似然估计是计算使得数据出现的可能性最大的参数**，依仗的自然是Probability。而**最小二乘是计算误差损失**。

### 1.5.7 逻辑回归与朴素贝叶斯有什么区别
（1）**逻辑回归是判别模型， 朴素贝叶斯是生成模型**，所以生成和判别的所有区别它们都有。
（2）**朴素贝叶斯属于贝叶斯，逻辑回归是最大似然**，两种概率哲学间的区别。
（3）朴素贝叶斯需要条件独立假设。
（4）逻辑回归需要求特征参数间是线性的。

## 1.6 代价函数

### 1.6.1 为什么需要代价函数
1. 为了得到训练模型的参数，需要一个代价函数，通过训练代价函数来得到参数。
2. 用于找到最优解的目的函数。
### 1.6.2 为什么代价函数要非负
目标函数存在一个下界，在优化过程当中，如果优化算法能够使目标函数不断减小，根据单调有界准则，这个优化算法就能证明是收敛有效的。只要设计的目标函数有下界，基本上都可以，代价函数非负更为方便。
### 1.6.3 常见代价函数
（1）**二次代价函数（quadratic cost）**：
$$
J = \frac{1}{2n}\sum_x\Vert y(x)-a^L(x)\Vert^2
$$
其中，$J$表示代价函数，$x$表示样本，$y$表示实际值，$a$表示输出值，$n$表示样本的总数。使用一个样本为例简单说明，此时二次代价函数为：
$$
J = \frac{(y-a)^2}{2}
$$
假如使用梯度下降法（Gradient descent）来调整权值参数的大小，权值$w$和偏置$b$的梯度推导如下：
$$
\frac{\partial J}{\partial b}=(a-y)\sigma'(z)
$$
其中，$z$表示神经元的输入，$\sigma$表示激活函数。权值$w$和偏置$b$的梯度跟激活函数的梯度成正比，激活函数的梯度越大，权值$w$和偏置$b$的大小调整得越快，训练收敛得就越快。

（2）**交叉熵代价函数（cross-entropy）**：
$$
J = -\frac{1}{n}\sum_x[y\ln a + (1-y)\ln{(1-a)}]
$$
其中，$J$表示代价函数，$x$表示样本，$y$表示实际值，$a$表示输出值，$n$表示样本的总数。
权值$w$和偏置$b$的梯度推导如下：
$$
\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;，
\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)
$$
当误差越大时，梯度就越大，权值$w$和偏置$b$调整就越快，训练的速度也就越快。

**二次代价函数适合输出神经元是线性的情况，交叉熵代价函数适合输出神经元是S型函数的情况。**

（3）**对数似然代价函数（log-likelihood cost）**：
$$
J =\frac{1}{N}\sum_{n-1}^N-ln\theta(y_nw^Tx_n)=\frac{1}{N}\sum_{n-1}^Nln(1+e^{-y_nw^Tx_n})
$$
对数似然函数常用来作为softmax回归的代价函数。
> 在tensorflow中：
		与sigmoid搭配使用的交叉熵函数：`tf.nn.sigmoid_cross_entropy_with_logits()`。
		与softmax搭配使用的交叉熵函数：`tf.nn.softmax_cross_entropy_with_logits()`。
在pytorch中：
      与sigmoid搭配使用的交叉熵函数：`torch.nn.BCEWithLogitsLoss()`。
		与softmax搭配使用的交叉熵函数：`torch.nn.CrossEntropyLoss()`。

### 1.6.4 为什么用交叉熵代替二次代价函数
（1）**为什么不用二次方代价函数**
由上一节可知，权值$w$和偏置$b$的偏导数
$$
\frac{\partial J}{\partial w}=(a-y)\sigma'(z)x \\ \frac{\partial J}{\partial b}=(a-y)\sigma'(z)
$$
偏导数受激活函数的导数影响，sigmoid函数导数在输出接近0和1时非常小，会导致一些实例在刚开始训练时学习得非常慢。
（2）**为什么要用交叉熵**
交叉熵函数权值$w$和偏置$b$的梯度推导为：
$$
\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;，
\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)
$$
由以上公式可知，权重学习的速度受到$\sigma{(z)}-y$影响，更大的误差，就有更快的学习速度，避免了二次代价函数方程中因$\sigma'{(z)}$导致的学习缓慢的情况。

## 1.7 损失函数

### 1.7.1 什么是损失函数

损失函数（Loss Function）又叫做误差函数，用来衡量算法的运行情况，估量模型的预测值与真实值的不一致程度。损失函数越小，模型的鲁棒性就越好。（实际应用中，损失函数与代价函数是相同的）
- 损失函数用来评价预测值和真实值不一样的程度。通常损失函数越好，模型的性能也越好。
- 损失函数可分为**经验风险损失函数和结构风险损失函数**。==经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是在经验风险损失函数上加上正则项==。
- 损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。

### 1.7.2 常见的损失函数

（1）**0-1损失函数**
如果预测值和目标值相等，值为0，如果不相等，值为1。
$$
L(Y, f(x)) =
\begin{cases}
1,& Y\ne f(x)\\
0,& Y = f(x)
\end{cases}
$$
一般的在实际使用中，相等的条件过于严格，可适当放宽条件：
$$
L(Y, f(x)) =
\begin{cases}
1,& |Y-f(x)|\geqslant T\\
0,& |Y-f(x)|< T
\end{cases}
$$
（2）**绝对值损失函数**
$$
L(Y, f(x)) = |Y-f(x)|
$$
（3）**平方损失函数**
$$
L(Y, f(x)) = \sum_N{(Y-f(x))}^2
$$
（4）**对数损失函数**
$$
L(Y, P(Y|X)) = -\log{P(Y|X)}
$$
常见的==逻辑回归使用的就是对数损失函数==。
（6）**指数损失函数**
指数损失函数的标准形式为：
$$
L(Y, f(x)) = \exp(-Yf(x))
$$
==AdaBoost就是以指数损失函数为损失函数==。
（7）**Hinge损失函数**
Hinge损失函数的标准形式如下：
$$
L(y) = \max{(0, 1-ty)}
$$
统一的形式：
$$
L(Y, f(x)) = \max{(0, Yf(x))}
$$
其中y是预测值，范围为(-1,1)，t为目标值，其为-1或1。
在线性支持向量机中，最优化问题可等价于
$$
\underset{\min}{w,b}\sum_{i=1}^N (1-y_i(wx_i+b))+\lambda\Vert w\Vert ^2
$$
上式相似于下式
$$
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i) + \Vert w\Vert ^2
$$
其中**$l(wx_i+by_i)$是Hinge损失函数，$\Vert w\Vert ^2$可看做为正则化项。**
## 1.8 梯度下降

关于梯度的概念：
（1）梯度是一个向量，即有方向有大小。 
（2）梯度的方向是最大方向导数的方向。 
（3）梯度的值是最大方向导数的值。
### 1.8.1 机器学习中为什么需要梯度下降

（1）梯度下降是迭代法的一种，可以用于求解最小二乘问题。在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。
（2）在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。
（3）如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。
### 1.8.2 梯度下降法缺点

（1）靠近极小值时收敛速度减慢。
（2）直线搜索时可能会产生一些问题。
（3）可能会“之字形”地下降。
### 1.8.3 梯度下降法算法描述

**核心思想归纳**：
（1）初始化参数，随机选取取值范围内的任意数；
（2）迭代操作：
	a）计算当前梯度；
	b）修改新的变量；
	c）计算朝最陡的下坡方向走一步；
	d）判断是否需要终止，如否，返回a）；
（3）得到全局最优解或者接近全局最优解。

梯度下降法算法步骤如下：
（1）确定优化模型的假设函数及损失函数。
​	举例，对于线性回归，假设函数为：
$$
h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n=\sum_{i=0}^n\theta_ix_i
$$
  其中，$\theta_i,x_i(i=0,1,2,...,n)$分别为模型参数、每个样本的特征值。
  对于假设函数，损失函数为：
$$
J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^{m}_{i=0}(h_\theta (x^{(i)})-y^{(i)})^2
$$
（2）相关参数初始化。
​	主要初始化${\theta}_i$、算法迭代步长${\alpha} $、终止距离${\zeta} $。初始化时可以根据经验初始化，即${\theta} $初始化为0，步长${\alpha} $初始化为1。当前步长记为${\varphi}_i $。当然，也可随机初始化。
（3）迭代计算。
​	1）计算当前位置时损失函数的梯度，对${\theta}_i $，其梯度表示为：
$$
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{(j)}_0
	,x^{(j)}_1,...,x^{(j)}_n)-y_j)x^{(j)}_i
$$
​	2）计算当前位置下降的距离。
$$
{\varphi}_i={\alpha} \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)
$$
​	3）判断是否终止。
​	确定是否所有${\theta}_i$梯度下降的距离${\varphi}_i$都小于终止距离${\zeta}$，如果都小于${\zeta}$，则算法终止，当然的值即为最终结果，否则进入下一步。
$$
{\theta}_i={\theta}_i-\alpha \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)
$$
$$
\theta_i=\theta_i - \alpha \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{(j)}_0
	,x^{(j)}_1,...,x^{(j)}_n)-y_j)x^{(j)}_i
$$
​	5）令上式$x^{(j)}_0=1$，更新完毕后转入1)。
​	由此，可看出，当前位置的梯度方向由所有样本决定。
### 1.8.4 随机梯度和批量梯度

随机梯度下降（SDG）和批量梯度下降（BDG）是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。
下面通过介绍两种梯度下降法的求解思路，对其进行比较。
假设函数为：
$$
h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n
$$
损失函数为：
$$
J(\theta_0, \theta_1, ... , \theta_n) = 
			\frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)^2
$$
其中，$m$为样本个数，$j$为参数个数。

1、 **批量梯度下降的求解思路如下：**
a) 得到每个$ \theta $对应的梯度：
$$
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$
b) 由于是求最小化风险函数，所以按每个参数 $ \theta $ 的梯度负方向更新 $ \theta_i $ ：
$$
\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$
c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都**要用到训练集所有的数据**，如果样本数据很大，这种方法迭代速度就很慢。
相比而言，随机梯度下降可避免这种问题。

2、**随机梯度下降的求解思路如下：**
a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是训练集中每个样本的粒度。
损失函数可以写成如下这种形式，
$$
J(\theta_0, \theta_1, ... , \theta_n) = 
			\frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0
			,x^{j}_1,...,x^{j}_n))^2 = 
			\frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))
$$
b）对每个参数 $ \theta$ 按梯度方向更新 $ \theta$：
$$
\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))
$$
c) 随机梯度下降是**通过每个样本来迭代更新一次**。
随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。

**小结：**
随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：

|     方法     | 特点                                                         |
| :----------: | :----------------------------------------------------------- |
| 批量梯度下降 | a）采用所有数据来梯度下降。<br/>b）批量梯度下降法在样本量很大的时候，训练速度慢。 |
| 随机梯度下降 | a）随机梯度下降用一个样本来梯度下降。<br/>b）训练速度很快。<br />c）随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br />d）收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。 |

下面介绍能结合两种方法优点的小批量梯度下降法。

3、 **小批量（Mini-Batch）梯度下降的求解思路如下**
对于总数为$m$个样本的数据，根据样本的数据，选取其中的$n(1< n< m)$个子样本来迭代。其参数$\theta$按梯度方向更新$\theta_i$公式如下：
$$
\theta_i = \theta_i - \alpha \sum^{t+n-1}_{j=t}
		( h_\theta (x^{j}_{0}, x^{j}_{1}, ... , x^{j}_{n} ) - y^j ) x^{j}_{i}
$$

代码：
```python
import numpy as np
# 数据训练数据
x = np.arange(0., 10., 0.2)
nums = x.shape[0]
b = np.full(nums, 1.)
input_data = np.vstack((b, x)).T
target_data = 2 * x + 5 + np.random.randn(nums)
# 终止条件
loop_max = 1000
epsilon = 1e-5

# 权重初始化
np.random.seed(0)
w = np.random.randn(2)
# 学习率
alpha = 0.001

# 随机梯度下降法（SGD）
def func_SGD(input_data, target_data, alpha=alpha, epsilon=epsilon, loop_max=loop_max):  
    count = 0
    pre_w = w
    while count < loop_max:
	    for i in range(input_data.shape[0]):
	        diff = target_data[i] - np.dot(input_data[i], w)
	        gradient = np.dot(input_data[i].T, diff)
	        w = w + alpha * gradient
	        if np.linalg.norm(w, pre_w) < epsilon:
		        break
	        else:
		        pre_w = w
	    count += 1
# 批量梯度下降法（BGD）
def func_BGD(input_data, target_data, alpha=alpha, epsilon=epsilon, loop_max=loop_max):
    count = 0
    pre_w = w
    while count < loop_max:
        gradient_total = 0
	    for i in range(input_data.shape[0]):
	        diff = target_data[i] - np.dot(input_data[i], w)
	        gradient += np.dot(input_data[i].T, diff)
	    w = w + alpha * gradient_total / input_data.shape[0]
	    if np.linalg.norm(w, pre_w) < epsilon:
	        break
	    else:
	        pre_w = w
	    count += 1
# 小批量梯度下降法（MBGD）
def func_MSGD(input_data, target_data, batch_size=batch_size, alpha=alpha, epsilon=epsilon, loop_max=loop_max):
    count = 0
    pre_w = w
    while count < loop_max:
        N = np.ceil(input_data.shape[0]*1.0 / batch_size)
        for i in range(N):
            if i < N - 1:
                input_temp = input_data[i*batch_size: (i+1)*batch_size]
                target_temp = target_data[i*batch_size: (i+1)*batch_size] 
            else:
                input_temp = input_data[i*batch_size: input_size.shape[0]]
                target_temp = target_data[i*batch_size: input_size.shape[0]]
            gradient_all = 0
            for j in range(input_temp.shape[0]):
                diff = target_temp[j] - np.dot(input_temp[j], w)
                gradient += np.dot(input_temp[j].T, diff)
            w = w + alpha * gradient/input_temp.shape[0]
            if np.linalg.norm(w, pre_w) < epsilon:
     		    break
 	        else:
     		    pre_w = w
        count += 1
```
> 介绍一下Online GD。
​	Online GD于Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。
​	Online GD在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的BGD算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。

### 1.8.5 各种梯度下降法性能比较

​	下表简单对比随机梯度下降（SGD）、批量梯度下降（BGD）、小批量梯度下降（Mini-batch GD）、和Online GD的区别：

||BGD|SGD|Mini-batch GD|Online GD|
|:-:|:-:|:-:|:-:|:-:|:-:|
|训练集|固定|固定|固定|实时更新|
|单次迭代样本数|整个训练集|单个样本|训练集的子集|根据具体算法定|
|算法复杂度|高|低|一般|低|
|时效性|低|一般|一般|高|
|收敛性|稳定|不稳定|较稳定|不稳定|

### 1.8.6 如何对梯度下降法进行调优

（1）**算法迭代步长$\alpha$选择。**
在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。
（2）**参数的初始值选择。**
初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。
（3）**标准化处理。**
由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间。

## 1.9 线性判别分析（LDA）

### 1.9.1 LDA思想总结

线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。
LDA分类思想简单总结如下：  
1. 多维空间中，数据处理分类问题较为复杂，LDA算法将**多维空间中的数据投影到一条直线上**，将d维数据转化成1维数据进行处理。  
2. 对于训练数据，设法将多维数据投影到一条直线上，**同类数据的投影点尽可能接近，异类数据点尽可能远离**。  
3. 对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。  

如果用一句话概括LDA思想，即“==投影后类内方差最小，类间方差最大==”。
### 1.9.2 图解LDA核心思想

假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。
![1.9.2](/1.9.2.png)
左图和右图是两种不同的投影方式。

- 左图思路：让不同类别的平均点距离最远的投影方式。
- 右图思路：让同类别的数据挨得最近的投影方式。
- 从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。
- 以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。

### 1.9.3 二类LDA算法原理

​	输入：数据集 $D=\{(\boldsymbol x_1,\boldsymbol y_1),(\boldsymbol x_2,\boldsymbol y_2),...,(\boldsymbol x_m,\boldsymbol y_m)\}$，其中样本 $\boldsymbol x_i $ 是n维向量，$\boldsymbol y_i  \epsilon \{0, 1\}$，降维后的目标维度 $d$。定义

​	$N_j(j=0,1)$ 为第 $j$ 类样本个数；

​	$X_j(j=0,1)$ 为第 $j$ 类样本的集合；

​	$u_j(j=0,1)$ 为第 $j$ 类样本的均值向量；

​	$\sum_j(j=0,1)$ 为第 $j$ 类样本的协方差矩阵。

​	其中
$$
u_j = \frac{1}{N_j} \sum_{\boldsymbol x\epsilon X_j}\boldsymbol x(j=0,1)， 
\sum_j = \sum_{\boldsymbol x\epsilon X_j}(\boldsymbol x-u_j)(\boldsymbol x-u_j)^T(j=0,1)
$$
​	假设投影直线是向量 $\boldsymbol w$，对任意样本 $\boldsymbol x_i$，它在直线 $w$上的投影为 $\boldsymbol w^Tx_i$，两个类别的中心点 $u_0$, $u_1 $在直线 $w$ 的投影分别为 $\boldsymbol w^Tu_0$ 、$\boldsymbol w^Tu_1$。

​	LDA的目标是==让两类别的数据中心间的距离 $\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2$ 尽量大==，与此同时，希望==同类样本投影点的协方差$\boldsymbol w^T \sum_0 \boldsymbol w$、$\boldsymbol w^T \sum_1 \boldsymbol w$ 尽量小，最小化 $\boldsymbol w^T \sum_0 \boldsymbol w - \boldsymbol w^T \sum_1 \boldsymbol w$== 。
​	定义
​	类内散度矩阵
$$
S_w = \sum_0 + \sum_1 = 
	\sum_{\boldsymbol x\epsilon X_0}(\boldsymbol x-u_0)(\boldsymbol x-u_0)^T + 
	\sum_{\boldsymbol x\epsilon X_1}(\boldsymbol x-u_1)(\boldsymbol x-u_1)^T
$$
​	类间散度矩阵
$$
S_b = (u_0 - u_1)(u_0 - u_1)^T
$$

​	据上分析，优化目标为
$$
\mathop{\arg\max}_\boldsymbol w J(\boldsymbol w) = \frac{\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2}{\boldsymbol w^T \sum_0\boldsymbol w + \boldsymbol w^T \sum_1\boldsymbol w} = 
\frac{\boldsymbol w^T(u_0-u_1)(u_0-u_1)^T\boldsymbol w}{\boldsymbol w^T(\sum_0 + \sum_1)\boldsymbol w} =
\frac{\boldsymbol w^TS_b\boldsymbol w}{\boldsymbol w^TS_w\boldsymbol w}
$$
​	根据广义瑞利商的性质，==矩阵 $S^{-1}_{w} S_b$ 的最大特征值为 $J(\boldsymbol w)$ 的最大值，矩阵 $S^{-1}_{w} S_b$ 的最大特征值对应的特征向量即为 $\boldsymbol w$==。

### 1.9.4 LDA算法流程总结

LDA算法降维流程如下：

​	输入：数据集 $D = \{ (x_1,y_1),(x_2,y_2), ... ,(x_m,y_m) \}$，其中样本 $x_i $ 是n维向量，$y_i  \epsilon \{C_1, C_2, ..., C_k\}$，降维后的目标维度 $d$ 。

​	输出：降维后的数据集 $\overline{D} $ 。

步骤：
1. 计算类内散度矩阵 $S_w$。
2. 计算类间散度矩阵 $S_b$ 。
3. 计算矩阵 $S^{-1}_wS_b$ 。
4. 计算矩阵 $S^{-1}_wS_b$ 的最大的 d 个特征值。
5. 计算 d 个特征值对应的 d 个特征向量，记投影矩阵为 W 。
6. 转化样本集的每个样本，得到新样本 $P_i = W^Tx_i$ 。
7. 输出新样本集 $\overline{D} = \{ (p_1,y_1),(p_2,y_2),...,(p_m,y_m) \}$

### 1.9.5 LDA和PCA区别

|异同点|LDA|PCA|
|:-:|:-|:-|
|相同点|1. 两者均可以对数据进行降维；<br />2. 两者在降维时均使用了矩阵特征分解的思想；<br />3. 两者都假设数据符合高斯分布；||
|不同点|有监督的降维方法；|无监督的降维方法；|
||降维最多降到k-1维；|降维多少没有限制；|
||可以用于降维，还可以用于分类；|只用于降维；|
||选择分类性能最好的投影方向；|选择样本点投影具有最大方差的方向；|
||更明确，更能反映样本间差异；|目的较为模糊；|

### 1.9.6 LDA优缺点

|优缺点|简要说明|
|:-:|:-|
|优点|1. 可以使用类别的先验知识；<br />2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；|
|缺点|1. LDA不适合对非高斯分布样本进行降维；<br />2. LDA降维最多降到分类数k-1维；<br />3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好；<br />4. LDA可能过度拟合数据。|

## 1.10 主成分分析（PCA）

PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将m维特征映射到n维（n < m），这n维形成主元，是重构出来最能代表原始数据的正交特征。
### 1.10.1 PCA思想总结

PCA就是将高维的数据通过线性变换投影到低维空间上去。
投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。
- 去冗余：去除**可以被其他向量代表的线性相关向量**，这部分信息量是多余的。
- 去噪声，去除**较小特征值对应的特征向量**，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。
- 对角化矩阵，==寻找极大线性无关组，保留较大的特征值，去除较小特征值==，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。

完成PCA的关键是——**协方差矩阵**。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。**协方差矩阵度量的是维度与维度之间的关系**，而非样本与样本之间。
之所以对角化，因为**对角化之后非对角上的元素都是0，达到去噪声的目的**。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们**只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余**。

### 1.10.2 图解PCA核心思想

假设数据集是m个n维，$(\boldsymbol x^{(1)}, \boldsymbol x^{(2)}, \cdots, \boldsymbol x^{(m)})$。如果$n=2$，需要降维到$n'=1$，现在想找到某一维度方向代表这两个维度的数据。下图有$u_1, u_2$两个向量方向，但是哪个向量才是我们所想要的，可以更好代表原始数据集的呢？
![1.9.2](/1.10.2.png)
从图可看出，$u_1$比$u_2$好，为什么呢？有以下两个主要评价指标：

1. 样本点到这个直线的距离足够近。
2. 样本点在这个直线上的投影能尽可能的分开。

如果我们需要降维的目标维数是其他任意维，则：
1. ==样本点到这个超平面的距离足够近。==
2. ==样本点在这个超平面上的投影能尽可能的分开。==

### 1.10.3 PCA算法推理

下面以==**基于最小投影距离为评价指标推理**==：

​	假设数据集是m个n维，$(x^{(1)}, x^{(2)},...,x^{(m)})$，且数据进行了中心化。经过投影变换得到新坐标为 ${w_1,w_2,...,w_n}$，其中 $w$ 是标准正交基，即 $\| w \|_2 = 1$，$w^T_iw_j = 0$。

​	经过降维后，新坐标为 $\{ w_1,w_2,...,w_n \}$，其中 $n'$ 是降维后的目标维数。样本点 $x^{(i)}$ 在新坐标系下的投影为 $z^{(i)} = \left(z^{(i)}_1, z^{(i)}_2, ..., z^{(i)}_{n'}   \right)$，其中 $z^{(i)}_j = w^T_j x^{(i)}$ 是 $x^{(i)} $ 在低维坐标系里第 j 维的坐标。

​	如果用 $z^{(i)} $ 去恢复 $x^{(i)} $ ，则得到的恢复数据为 $\widehat{x}^{(i)} = \sum^{n'}_{j=1} x^{(i)}_j w_j = Wz^{(i)}$，其中 $W$为标准正交基组成的矩阵。

​	考虑到整个样本集，**样本点到这个超平面的距离足够近，目标变为最小化 $\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2$ **。对此式进行推理，可得：
$$
\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2 = 
	\sum^m_{i=1} \| Wz^{(i)} - x^{(i)} \|^2_2 \\
	= \sum^m_{i=1} \left( Wz^{(i)} \right)^T \left( Wz^{(i)} \right)
	- 2\sum^m_{i=1} \left( Wz^{(i)} \right)^T x^{(i)}
	+ \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
	= \sum^m_{i=1} \left( z^{(i)} \right)^T \left( z^{(i)} \right)
	- 2\sum^m_{i=1} \left( z^{(i)} \right)^T x^{(i)}
	+ \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
	= - \sum^m_{i=1} \left( z^{(i)} \right)^T \left( z^{(i)} \right)
	+ \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
	= -tr \left( W^T \left( \sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T \right)W \right)
	+ \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
	= -tr \left( W^TXX^TW \right)
	+ \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)}
$$

​	在推导过程中，分别用到了 $\overline{x}^{(i)} = Wz^{(i)}$ ，矩阵转置公式 $(AB)^T = B^TA^T$，$W^TW = I$，$z^{(i)} = W^Tx^{(i)}$ 以及矩阵的迹，最后两步是将代数和转为矩阵形式。
​	由于 $W$ 的每一个向量 $w_j$ 是标准正交基，$\sum^m_{i=1} x^{(i)} \left(  x^{(i)} \right)^T$ 是数据集的协方差矩阵，$\sum^m_{i=1} \left(  x^{(i)} \right)^T x^{(i)} $ 是一个常量。最小化 $\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2$ 又可等价于
$$
\underbrace{\arg \min}_W - tr \left( W^TXX^TW \right) s.t.W^TW = I
$$
利用拉格朗日函数可得到
$$
J(W) = -tr(W^TXX^TW) + \lambda(W^TW - I)
$$
对 $W$ 求导，可得 
$$
-XX^TW + \lambda W = 0 \\ XX^TW = \lambda W
$$
$ XX^T $ 是 $ n' $ 个特征向量组成的矩阵，$\lambda$ 为$ XX^T $ 的特征值。$W$ 即为我们想要的矩阵。
	然后只要对协方差矩阵$XX^T$进行特征分解，对求得的特征值$\lambda$ 进行排序，对于原始数据，需要 $z^{(i)} = W^TX^{(i)}$ ，就可把原始数据集降维到最小投影距离的 $n'$ 维数据集$Y$。

下面以==**基于最大方差为评价指标推理**==：
先解释一下投影的概念：
![1.9.3](/1.10.3.png)
红色点表示样例$x^{(i)}$,蓝色点表示$x^{(i)}$在$u$上的投影，$u$是直线的斜率也是直线的方向向量，而且是单位向量。 蓝色点是$x^{(i)}$在$u$上的投影点，离原点的距离是$<x^{(i)},u>$（即$x^{(i)^T}u$或者$u^Tx^{(i)}$）由于这些样本点（样例）的每一维特征均值都为0，因此投影到$u$上的样本点（只有一个到原点的距离值）的均值仍然是0。  我们要求的是最佳的u，使得投影后的样本点方差最大。 
 由于投影后均值为0，因此方差为： 
$$
\frac1m\sum_{i=1}^m(x^{(i)^T}u)=\frac1m\sum_{i=1}^mu^Tx^{(i)}x^{(i)^T}u=u^T(\frac1m\sum_{i=1}^mx^{(i)}x^{(i)^T})u
$$
 中间那部分很熟悉啊，不就是样本特征的协方差矩阵么。用$\lambda$来表示$\frac1m\sum_{i=1}^m(x^{(i)^T}u)$，$\sum$表示$\frac1m\sum_{i=1}^mx^{(i)}x^{(i)^T}$，那么上式改为：
$$
\lambda=u^T\sum u
$$
由于$u$是单位向量，即$u^Tu=1$，上面两边都左乘$u$得
$$
u\lambda=\lambda u=uu^T\sum u=\sum u \\
\sum u=\lambda u
$$
则$\lambda$就是$\sum$的特征值，$u$是特征向量。最佳的投影直线是特征值$\lambda$最大时对应的特征向量，其次是$\lambda$第二大对应的特征向量，依次类推。
 因此，我们只需要对协方差矩阵进行特征值分解，得到的前k大特征值对应的特征向量就是最佳的k维新特征，而且这k维新特征是正交的。得到前k个u以后，样例$x^{(i)}$通过以下变换可以得到新的样本。
$$
y^{(i)}=\begin{bmatrix} u^T_1x^{(i)} \\ u^T_2x^{(i)} \\ \vdots \\ u^T_kx^{(i)}\end{bmatrix}\in\mathbb{R}^k
$$
其中的第j维就是$x^{(i)}$在$u_j$上的投影。
通过选取最大的k个u，使得方差较小的特征（如噪声）被丢弃。

### 1.10.4 PCA算法流程总结

输入：$n$ 维样本集 $D = \left( x^{(1)},x^{(2)},...,x^{(m)} \right)$ ，目标降维的维数 $n'$ 。
输出：降维后的新样本集 $D'  = \left( z^{(1)},z^{(2)},...,z^{(m)} \right)$ 。
主要步骤如下：
1. 对所有的样本进行中心化，$ x^{(i)} = x^{(i)} - \frac{1}{m} \sum^m_{j=1} x^{(j)} $ 。
2. 计算样本的协方差矩阵 $XX^T$ 。
3. 对协方差矩阵 $XX^T$ 进行特征值分解。
4. 取出最大的 $n' $ 个特征值对应的特征向量 $\{ w_1,w_2,...,w_{n'} \}$ 。
5. 标准化特征向量，得到特征向量矩阵 $W$ 。
6. 转化样本集中的每个样本 $z^{(i)} = W^T x^{(i)}$ 。
7. 得到输出矩阵 $D' = \left( z^{(1)},z^{(2)},...,z^{(n')} \right)$ 。
*注*：在降维时，有时不明确目标维数，而是指定降维到的主成分比重阈值 $k(k \epsilon(0,1])$ 。假设 $n$ 个特征值为 $\lambda_1 \geqslant \lambda_2 \geqslant ... \geqslant \lambda_n$ ，则 $n'$ 可从 $\sum^{n'}_{i=1} \lambda_i \geqslant k \times \sum^n_{i=1} \lambda_i $ 得到。

### 1.10.5 PCA代码

```python
class PCA():
    def __init__(self, features):
        self.features = features
    def fit(self, k_dimension):
        assert k_dimension <= self.features.shape[1], '目标维度应小于原始维度'
        self.features -= self.features.mean(axis=0) # 去中心化
        print(self.features)
        cov = np.cov(self.features, rowvar=False) # 计算协方差
        eig_val, eig_vec = np.linalg.eig(cov) # 特征值分解
        eig_val_index = np.argsort(-eig_val)  # 从大到小
        eig_vec = eig_vec[:, eig_val_index[0:k_dimension]] # 特征向量矩阵计算
        data_reduced = np.dot(self.features, eig_vec)
        return data_reduced
```

### 1.10.6 PCA算法主要优缺点

|优缺点|简要说明|
|:-:|:-|
|优点|1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。<br/>2.各主成分之间正交，可消除原始数据成分间的相互影响的因素。<br/>3. 计算方法简单，主要运算是特征值分解，易于实现。|
|缺点|1.主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。<br/>2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。|

### 1.10.7 降维的必要性及目的

**降维的必要性**：
1. 多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。
2. 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。
3. 过多的变量，对查找规律造成冗余麻烦。
4. 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。

**降维的目的**：
1. 减少预测变量的个数。
2. 确保这些变量是相互独立的。
3. 提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。
4. 数据在低维下更容易处理、更容易使用。
5. 去除数据噪声。
6. 降低算法运算开销。

### 1.10.8 KPCA与PCA的区别

​		应用PCA算法前提是假设存在一个线性超平面，进而投影。那如果数据不是线性的呢？该怎么办？这时候就需要KPCA，数据集从 $n$ 维映射到线性可分的高维 $N >n$，然后再从 $N$ 维降维到一个低维度 $n'(n'<n<N)$ 。
​		KPCA用到了**核函数**思想，使用了核函数的主成分分析一般称为核主成分分析(Kernelized PCA, 简称KPCA）。
假设高维空间数据由 $n$ 维空间的数据通过映射 $\phi$ 产生
​	$n$ 维空间的特征分解为：
$$
\sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T W = \lambda W
$$
​	其映射为
$$
\sum^m_{i=1} \phi \left( x^{(i)} \right) \phi \left( x^{(i)} \right)^T W = \lambda W
$$
​	通过在高维空间进行协方差矩阵的特征值分解，然后用和PCA一样的方法进行降维。由于KPCA需要核函数的运算，因此它的计算量要比PCA大很多。

### 1.10.9 为什么会产生维数灾难
特征数量越多，训练样本就会越稀疏，分类器的参数估计就会越不准确，更加容易出现过拟合问题。“维数灾难”的另一个影响是训练样本的稀疏性并不是均匀分布的。处于中心位置的训练样本比四周的训练样本更加稀疏。

### 1.10.10  怎样避免维数灾难
**有待完善！！！**
解决维度灾难问题：
主成分分析法PCA，线性判别法LDA，奇异值分解简化数据、拉普拉斯特征映射，Lassio缩减系数法、小波分析法、

## 1.11 决策树
### 1.11.1 决策树的基本原理
决策树（Decision Tree）是一种分而治之的决策过程。一个困难的预测问题，通过树的分支节点，被划分成两个或多个较为简单的子集，从结构上划分为不同的子问题。将依规则分割数据集的过程不断递归下去（Recursive Partitioning）。随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题数也逐渐简化。当分支节点的深度或者问题的简单程度满足一定的停止规则（Stopping Rule）时, 该分支节点会停止分裂，此为自上而下的停止阈值（Cutoff Threshold）法；有些决策树也使用自下而上的剪枝（Pruning）法。

### 1.11.2 决策树的三要素
​	一棵决策树的生成过程主要分为下3个部分：  
​	1、特征选择：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。 
​	2、决策树生成：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。树结构来说，递归结构是最容易理解的方式。 
​	3、剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。

### 1.11.3 决策树学习基本算法

![1.11.3](/1.11.3.png)

### 1.11.4 决策树算法优缺点
**决策树算法的优点**：  
1、决策树算法易理解，机理解释起来简单。 
2、决策树算法可以用于小数据集。
3、决策树算法的时间复杂度较小，为用于训练决策树的数据点的对数。
4、相比于其他算法智能分析一种类型变量，决策树算法可处理数字和数据的类别。
5、能够处理多输出的问题。 
6、对缺失值不敏感。
7、可以处理不相关特征数据。
8、效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。

**决策树算法的缺点**： 
1、对连续性的字段比较难预测。
2、容易出现过拟合。
3、当类别太多时，错误可能就会增加的比较快。
4、在处理特征关联性比较强的数据时表现得不是太好。
5、对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征。

### 1.11.5 熵的概念以及理解
==熵：度量随机变量的不确定性。==  
定义：假设随机变量X的可能取值有$x_{1},x_{2},...,x_{n}$，对于每一个可能的取值$x_{i}$，其概率为$P(X=x_{i})=p_{i},i=1,2...,n$。随机变量的熵为：
$$
H(X)=-\sum_{i=1}^{n}p_{i}log_{2}p_{i}
$$
对于样本集合，假设样本有k个类别，每个类别的概率为$\frac{|C_{k}|}{|D|}$，其中 ${|C_{k}|}$为类别为k的样本个数，$|D|$为样本总数。样本集合D的熵为：
$$
H(D)=-\sum_{k=1}^{k}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}
$$

### 1.11.6 信息增益、信息增益比
==信息增益：以某特征划分数据集前后的熵的差值。== 
熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以**使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。** 
假设划分前样本集合D的熵为H(D)。使用某个特征A划分数据集D，计算划分后的数据子集的熵为H(D|A)。 则信息增益为：
$$
g(D,A)=H(D)-H(D|A)
$$
*注：*在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得**信息增益最大的特征**来划分当前数据集D。 
思想：计算所有特征划分数据集D，得到多个特征划分数据集D的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。 

信息增益比：==$信息增益比=惩罚参数\times信息增益$== 
特征A对训练集D的信息增益比$g_R(D,A)$定义为：信息增益$g(D,A)$与关于特征A的熵$H_A(D)$之比：
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}
$$
$H_A(D)$表征了特征A对训练集D的拆分能力。因为$H_A(D)$只考虑样本在特征A上的取值，而不考虑样本的标记 ，所以这种拆分并不是对样本的分类。
惩罚参数：数据集D以特征A作为随机变量的熵的倒数。
信息增益比本质：在信息增益的基础之上乘上一个惩罚参数。
- 当特征A的取值集合较大时，加权系数较小，表示抑制该特征。
- 当特征A的取值集合较小时，加权系数较大，表示鼓励该特征。

### 1.11.7 ID3算法、C4.5算法
ID3 生成算法核心是在决策树的每个结点上应用**信息增益**准则选择特征，递归地构建决策树。
- 从根结点开始，计算结点所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征划分出子结点。
- 再对子结点递归地调用以上方法，构建决策树。
- 直到所有特征的信息增益均很小或者没有特征可以选择为止，最后得到一个决策树 。

C4.5 生成算法与 ID3 算法相似，但是 C4.5 算法在生成过程中用**信息增益比**来选择特征。

### 1.11.8 剪枝处理

剪枝处理是决策树学习算法用来解决过拟合问题的一种办法。在决策树算法中，为了尽可能正确分类训练样本， 节点划分过程不断重复， 有时候会造成决策树分支过多，以至于将训练样本集自身特点当作泛化特点， 而导致过拟合。 因此可以采用剪枝处理来去掉一些分支来降低过拟合的风险。 

剪枝的依据是：极小化决策树的整体损失函数或者代价函数。
决策树生成算法是学习局部的模型，决策树剪枝是学习整体的模型。即：==生成算法仅考虑局部最优，而剪枝算法考虑全局最优==。

剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）。
- 预剪枝：在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能， 如果不能提升，则停止划分，将当前节点标记为叶结点。 
- 后剪枝：生成决策树以后，再自下而上对非叶结点进行考察， 若将此节点标记为叶结点可以带来泛化性能提升，则修改之。

剪枝算法：
- 输入：
	- 生成算法产生的决策树$T$
	- 参数$\alpha$：控制预测误差与模型复杂度之间的关系。
		- 较大的$\alpha$会选择较简单的模型 。
		- 较小的$\alpha$会选择较复杂的模型。
		- $\alpha=0$只考虑对训练集的拟合，不考虑模型复杂度。
	
- 输出：修剪后的决策树$T_\alpha$

- 算法步骤：
	- 对树$T$每个节点$T_t,t=1,2,...|T_f|$，$T_f$为叶节点数目，计算其经验熵$H(t)$
	设该叶节点有$N_t$个样本点，其中属于$c_k$类的样本点有$N_{t,k}$个，$\sum_{k=1}^{K}N_{t,k}=N_t$
	$$
	H(t)=-\sum_{k=1}^{K}\frac{N_{t,k}}{N_t}log\frac{N_{t,k}}{N_t}
	$$
	 		叶结点个数越多，表示决策树越复杂，则损失函数越大。
	 		叶结点经验熵越大，表示叶结点的样本类别分布很分散，则损失函数越大。
	 		叶结点经验熵还需要加权，权重为叶结点大小。即：越大的叶结点，其分类错误的影响越大。
	
	- 递归地从树的叶节点向上回退；
	  设一组叶节点回退到父节点之前与之后的整棵树分别为$T$与$T'$，对应的损失函数值分别为$C_\alpha(T)$与$C_\alpha(T')$。若$C_\alpha(T')\leq C_\alpha(T)$， 则进行剪枝，将父结点变成新的叶结点。
	  $$
	  C(T)=\sum_{t=1}^{|T_f|}N_tH(t)=-\sum_{t=1}^{|T_f|}\sum_{k=1}^KN_{t,k}log\frac{N_{t,k}}{N_t} \\ C_\alpha(T)=C(T)+\alpha |T_f|
	  $$
	  其中，$\alpha |T_f|$为正则化向，$C(T)$表示预测误差。
	
	  $C(T)=0$表示$N_{t,k}=N_t$，即每个节点$T_t$内的样本都是纯的，树$T$的每个叶子节点只有一个样本点，即单一的分类。这样的决策树其实是没有什么实用价值的，所以必须**使用正则化项来约束决策树的复杂程度**。
	
	  ==决策树的剪枝准则是：考虑当$\alpha$确定时，$C_\alpha(T)$最小化。这等价于正则化的极大似然估计。==
	
	- 递推回归知道不能继续为止，得到损失函数最小的子树$T_\alpha$

### 1.11.9 CART树
CART树（classfification and regression tree）定义：学习在给定输入随机变量$\vec x$条件下，输出随机变量$y$的条件概率分布的模型。它同样由特征选取，树的生成、剪枝组成。既可用于分类，也可用于回归。
CART 算法：
- 决策树生成：用训练数据生成尽可能大的决策树。
	假设决策树是二叉树，内部结点特征的取值为 是 与 否 。其中：左侧分支取 是，右侧分支取 否 。它递归地二分每个特征，将输入空间划分为有限个单元。
- 决策树剪枝：用验证数据基于损失函数最小化的标准对生成的决策树剪枝。

#### 1.11.9.1 CART 回归树
==生成准则：平方误差最小化准则==
生成算法：

- 输入：
	- 训练数据集$\mathbb D={(\vec x_1, \tilde y_1),(\vec x_2, \tilde y_2),...,(\vec x_N, \tilde y_N)}$
	- 停止条件
	
- 输出：CART树$f(\vec x)$
  设已经将输入控件划分为$M$个单元$R_1,R_2,...,R_M$，且在每个单元$R_m$上有一个固定的输出值$c_m$，则CART回归树模型可以表示为：其中$I(·)$为示性函数（$x \rightarrow \{0,1\}$)
  $$
  f(\vec x)=\sum_{m=1}^Mc_mI(\vec x\in R_m)
  $$

- 步骤：
	- 选择最优切分维度$j$和切分点$s$
	如果已知输入空间的单元划分，基于平方误差最小的准则，则CART回归树在训练数据集上的损失函数为：
	$$
	\sum_{m=1}^M\sum_{\vec x_i\in R_m}(\tilde y_i-c_m)^2
	$$
	根据损失函数最小，则可以求解出每个单元上的最优输出量$\hat c_m$为：$R_m$上所有输入样本$\vec x_i$对应的输出$\tilde y_i$的平均值。即$\hat c_m=\frac 1{N_m}\sum_{\vec x_i \in R_m}\tilde y_i$，其中$N_m$表示单元$R_m$中的样本数量。
	
	问题是输入空间的单元划分是未知的。如何对输入空间进行划分？
	 设输入为n维：$\vec x=(x_1,x_2,...x_n)^T$，选择第$j$维$x_j$和它的取值$s$作为切分变量和切分点。定义两个区域
	$$
	  R_1(j,s)=\{\vec x|x_j\leq s\} \\ R_2(j,s)=\{\vec x|x_j>s\}
	$$
	- 然后寻求最优切分变量$j$和最优切分点$s$，即求解
	$$
	(j^*,s^*)=\underset{j,s}{min}[\underset{c_1}{min}\sum_{\vec x_i\in R_1(j,s)}(\tilde y_i-c_1)^2+\underset{c_2}{min}\sum_{\vec x_i\in R_2(j,s)}(\tilde y_i-c_2)^2]
	$$
	其意义：
	首先假设已知切分变量$j$，则遍历最优切分点$s$，则到：
	$$
	\hat c_1=\frac1{N_1}\sum_{\vec x_i\in R_1(j,s)}\tilde y_i,\\ \hat c_2=\frac1{N_2}\sum_{\vec x_i\in R_2(j,s)}\tilde y_i
	$$
	其中$N_1$和$N_2$分别代表区域$R_1$和$R_2$中的样本数量。
	然后遍历所有的特征维度，对每个维度找到最优切分点。从这些（切分维度，最优切分点）中找到是的损失函数最小的那个。
	- 对子区域$R_1$，$R_2$递归地切分，直到满足停止条件。这样的回归树成为最小二乘回归树。
	
	- 最终将输入空间划分为$M$个区域$R_1,R_2,...R_M$，生成决策树：
	  $$
	  f(\vec x)=\sum_{m=1}^M\hat c_mI(\vec x\in R_m)
	  $$
	  

#### 1.11.9.2 CART 分类树（基尼系数）

==生成准则：基尼指数最小化准则==

**基尼系数**
假设有$K$个分类，样本属于第$k$类的概率为$p_k=p(y=c_k)$。则概率分布的基尼指数为：
$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$
==基尼指数表示：样本集合中，随机选中一个样本，该样本被分错的概率。基尼指数越小，表示越不容易分错。==

对于给定的样本集合$\mathbb D$，设属于类$c_k$的样本子集为$\mathbb D_k$，则样本集的基尼指数为：
$$
Gini(\mathbb D)=1-\sum_{k=1}^K(\frac{N_k}N)^2
$$
其中$N$为样本总数，$N_k$为子集$\mathbb D_k$的样本数量。

对于最简单的二项分布，设$p(X=1)=p,P(X=0)=1-p$，则其基尼系数与熵的图形如下图：

![1.11.9](/1.11.9.png)
可以看出，基尼系数与熵一样，也是度量不确定性的度量。对于样本集$\mathbb D$，$Gini(\mathbb D)$越小，说明集合中的样本越纯净。

若样本集$\mathbb D$根据特征$A$是否小于$a$而被分为两个子集：$\mathbb D_1$和$\mathbb D_2$，其中：
$$
\mathbb D_1=\{(\vec x,y)\in \mathbb D|x_A\leq a\}\\\mathbb D_2=\{(\vec x,y)\in \mathbb D|x_A> a\}=\mathbb D-\mathbb D_1
$$
则在特征$A$：$a$的条件下，集合$\mathbb D$的基尼指数为：
$$
Gini(\mathbb D,A:a)=\frac{N_1}NGini(\mathbb D_1)+\frac{N_2}NGini(\mathbb D_2)
$$
其中$N$为样本总数，$N_1$，$N_2$分别为$\mathbb D_1$，$\mathbb D_2$的样本数量。它就是每个子集的基尼系数的加权和，权重是每个子集的大小（以子集占总体集合大小的百分比来表示）。

生成算法：
- 输入：
	- 训练数据集$\mathbb D$
	- 停止计算条件
	
- 输出：CART决策树

- 步骤：
	- 选择最优切分维度$j$和切分点$s$
	如果已知输入空间的单元划分，基于分类误差最小的准则，则CART分类树在训练数据集上的损失函数为
	$$
	\sum_{m=1}^M\sum_{\vec x_i\in R_m}I(\tilde y_i\neq c_m)
	$$
	根据损失函数最小，则可以求解出每个单元上的最优输出值$\hat c_m$为：$R_m$上所有输入样本$\vec x_i$对应的输出$\tilde y_i$的众数。即
	$$
	\hat c_m=argmax_{c_m}\sum_{\vec x_i\in R_m}I(c_m=\tilde y_i)
	$$
	问题是输入空间的单元划分是未知的。如何对输入空间进行划分？
	类似CART 回归树， CART 分类树遍历所有可能的维度$j$和该维度所有可能的取值$s$，取使得基尼系数最小的那个维度$j$和切分点$s$。
	即求解：
	$$
	(j^*,s^*)=min_{j,s}Gini(\mathbb D,A_j=s)
	$$
	它表示：遍历所有可能的维度$j$和该维度所有可能的取值$s$，取使得基尼系数最小的那个维度$j$和切分点$s$。
	- 用选定的$(j,s)$划分区域并决定响应的输出值
	$$
	R_1(j,s)=\{\vec x|x_j\leq s\},R_2(j,s)=\{\vec x|x_j>s\} \\ \hat c_1=arg\underset{c_1}{max}\sum_{\vec x_i\in R_1}I(c_1=\tilde y_i),\hat c_2=arg\underset{c_2}{max}\sum_{\vec x_i\in R_2}I(c_2=\tilde y_i)
	$$
	其中$N_1$和$N_2$分别代表区域$R_1$和$R_2$中的样本数量。
	
	- 对子区域$R_1$，$R_2$递归地切分，直到满足停止条件；
	
	- 最终将输入空间划分为$M$个区域$R_1,R_2,...R_M$，生成决策树：
	  $$
	  f(\vec x)=\sum_{m=1}^M\hat c_mI(\vec x\in R_m)
	  $$
#### 1.11.9.3 其他讨论
1. CART 分类树和CART 回归树通常的停止条件为：
	- 结点中样本个数小于预定值，这表示树已经太复杂。
	- 样本集的损失函数或者基尼指数小于预定值，表示结点已经非常纯净。
	- 没有更多的特征可供切分。
	
2. 前面讨论的CART 分类树和CART 回归树都假设特征均为连续值。实际上CART 树的特征可以为离散值，此时切分区域定义为：
   $$
   R_1(j,s)=\{\vec x|x_j=s\} \\ R_2(j,s)=\{\vec x|x_j \neq s\}
   $$
   连续的特征也可以通过分桶来进行离散化，然后当作离散特征来处理。

#### 1.11.9.4 CART剪枝
CART 树的剪枝是从完全生长的CART 树底端减去一些子树，使得CART 树变小（即模型变简单），从而使得它对未知数据有更好的预测能力。

原理：
1. 定义CART树的损失函数为$(\alpha \geq 0)$：$C_\alpha(T)=C(T)+\alpha|T_f|$。其中，$C_\alpha(T)$为参数是$\alpha$时树$T$的整体损失；$C(T)$为树$T$对训练数据的预测误差；$|T_f|$为子树的叶节点个数。
2. 对于固定的$\alpha$，存在使$C_\alpha(T)$最小的子树，令其为$T_\alpha^*$。可以证明$T_\alpha^*$是唯一的。
	- 当$\alpha$大时，$T_\alpha^*$偏小，即叶节点偏少；
	- 当$\alpha$小时，$T_\alpha^*$偏大，即叶节点偏多；
	- 当$\alpha=0$时，未剪枝的生成树就是最优的，此时不需要剪枝；
	- 当$\alpha=\infty$时，根节点组成的一个单节点树就是最优的，此时剪枝已经到极致，只剩一个节点；
3. 令从生成树$T_0$开始剪枝。对$T_0$任意非叶节点$t$：
   - 以$t$为单节点树：因为此时只有一个叶节点，即为$t$本身，所以损失函数为：
     $$
     C_\alpha(t)=C(t)+\alpha
     $$
   - 以$t$为根的子树$T_t$：此时的损失函数为
   $$
   C_\alpha(T_t)=C(T_t)+\alpha|T_t|
   $$
4. 可以证明
	- 当$\alpha=0$及充分小时，有$C_\alpha(T_t)<C_\alpha(t)$。即此时倾向于选择比较复杂的$T_t$，因为正则化项的系数$\alpha$太小；
	- 当$\alpha$增大到某个值是，有$C_\alpha(T_t)=C_\alpha(t)$;
	- 当$\alpha$再增大时，有$C_\alpha(T_t)>C_\alpha(t)$。即此时倾向于选择比较简单的$t$，因为正则化的系数$\alpha$太大。
5. 令$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$，此时$T_t$与$t$有相同的损失函数值，但是$t$的结点更少。因此$t$比$T_t$更可取，于是对$T_t$进行剪枝。
6. 对$T_0$内部的每一个内部节点$t$，计算$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$。它表示剪枝后整体损失函数增加的程度（可以为正，可以为负）。则有：
$$
C_\alpha(t)-C_\alpha(T_t)=C(t)+\alpha-C(T_t)-\alpha|T_t| \\ =C(t)-C(T_t)-\alpha(|T_t|-1)=(g(t)-\alpha)(|T_t|-1)
$$

​		因为$t$是个内部点，所以$|T_t|>1$，因此有：
​	- $g(t)>\alpha$时，$C_\alpha(t)-C_\alpha(T_t)>0$，表示剪枝后，损失函数增加；
​	- $g(t)=\alpha$时，$C_\alpha(t)-C_\alpha(T_t)=0$，表示剪枝后，损失函数不变；
​	- $g(t)<\alpha$时，$C_\alpha(t)-C_\alpha(T_t)<0$，表示剪枝后，损失函数减少；

7. 对$T_0$内部的每一个内部节点$t$，计算最小的$g$：
$$
g^*=min g(t),t\in T_0 and t is not a leaf
$$

​	设$g^*$对应的内部结点为$t^*$，在$T_0$内减去$t^*$，得到的子树作为$T_1$。令$\alpha_1=g^*$，对于$t\neq t^*$，有：
$$
g(t)>g(t^*)=\alpha_1,t \in T_0 and t is not a leaf and t \neq t^*
$$
对于$\alpha>alpha_1$，有：
	- 对于$t^*$剪枝，得到的子树的损失函数一定是减少的。它也是所有内部结点剪枝结果中，减少的最多的。因此$T_1$是$\alpha\in [\alpha_1,)$内的最优子树；
	- 对任意一个非$t^*$内部结点的剪枝，得到的子树的损失函数有可能是增加的，也可能是减少的。如果损失函数是减少的，它也没有$T_1$减少的多。
8. 如此剪枝下去，直到根节点被剪枝：
	- 此过程中不断产生$\alpha_i$的值，产生新区间$[\alpha_1,\alpha_2),[\alpha_2,\alpha_3),...$
	- 此过程中不断产生最优子树$T_1,T_2,...$
	- 其中$T_1$是由$T_0$产生的、$\alpha\in [\alpha_1,\alpha_2)$内的最优子树；$T_2$是由$T_1$产生的、$\alpha\in [\alpha_2,\alpha_3)$内的最优子树；...
9. 上述剪枝的思想就是用递归的方法对树进行剪枝：计算出一个序列$0=\alpha_0<\alpha_1<...<\alpha_n<\infty$，同时剪枝得到一系列最优子树序列$\{T_0,T_1,...T_n\}$。其中$T_i$是$\alpha\in [\alpha_i,\alpha_i+1)$时的最优子树。
10. 上述剪枝的结果只是对于训练集的损失函数较小。
	- 需要用交叉验证的方法在验证集上对子树序列进行测试，挑选中出最优子树。交叉验证的本质就是为了挑选超参数$\alpha$。
	- 验证过程：用独立的验证数据集来测试子树序列$\{T_0,T_1,...T_n\}$中各子树的平方误差或者基尼指数。由于$\{T_0,T_1,...T_n\}$对应于一个参数序列$\{\alpha_1,\alpha_2,...\alpha_n\}$，因此当最优子树$T_k$确定时，对应的区间$[\alpha_k,\alpha_{k+1})$也确定了。

CART 剪枝由两步组成：

- 从生成算法产生的决策树$T_0$底端开始不断地剪枝：
	- 每剪枝一次生成一个决策树$T_i,i=1,2,...$
	- 这一过程直到$T_0$的根节点，形成一个子树序列$\{T_0,T_1,...T_n\}$

CART剪枝算法：
- 输入：CART算法生成的决策树$T_0$
- 输出：最优决策树$T^*$
- 算法步骤：
	- 初始化：$k=0,T=T_0,\alpha=\infty$
	- 自下而上的对各内部节点$t$计算：$C(T_t),|T_t|,g(t)=\frac{C(t)-C(T_t)}{|T_t|-1},\alpha=min(\alpha,g(t))$
	- 自下而上地访问内部节点$t$：若有$g(t)=\alpha$，则进行剪枝，并确定叶节点$t$的输出，得到树$T$
		- 如果为分类树，则叶节点$t$的输出采取多数表决法：结点$t$内所有样本的标记的众数；
		- 如果为回归树，则叶节点$t$的输出为平均法：结点$t$内所有样本的标记的均值
	- 令$k=k+1,\alpha_k=\alpha,T_k=T$
	- 若$T$不是根节点单独构成的树，则继续前面的步骤
	- 采用交叉验证法在子树序列$T_0,T_1,...T_n$中选取最优子树$T^*$

### 1.11.10 CART 树与ID3 决策树和 C4.5 决策树的重要区别

- CART 树是二叉树，而后两者是N 叉树。
	由于是二叉树，因此 **CART 树的拆分不依赖于特征的取值数量**。因此CART 树也就不像ID3 那样倾向于取值数量较多的特征。
- CART 树的特征可以是离散的，也可以是连续的。而后两者的特征是离散的。如果是连续的特征，则需要执行分桶来进行离散化。
	CART 树处理连续特征时，也可以理解为二分桶的离散化。
	
### 1.11.11 连续值和缺失值处理
**连续值**
1. 使用连续属性离散化技术将连续特征转换为离散特征。最常用的离散化技术为二分法**bi-partition** ：
	给定样本集$\mathbb D$和连续属性$A$，假设该属性在$\mathbb D$中出现了$M$个不同的取值，将这些值从小到大进行排序，记作$a_1,a_2,...a_M$。选取$M-1$个划分点，依次为：$\frac{a_1+a_2}{2},\frac{a_2+a_3}{2},...\frac{a_{M-1}+a_M}{2}$。然后就可以像离散属性值一样来考察这些划分点，选取最优的划分点进行样本集和的划分。
2. 这也是 C4.5 算法采取的方案。
3. 事实上划分点的数量可以是任意正整数，划分点的位置也可以采取其它的形式。划分点的数量、划分点的位置都是超参数，需要结合验证集、具体问题来具体分析。

**缺失值**
1. 样本的某些属性值缺失。如果简单地放弃不完整样本，仅使用无缺失值的样本来进行学习，则是对数据信息的极大浪费。

2. 这里有两个问题：
	- 如何在属性值缺失的情况下选择划分属性？
	- 给定划分属性，如果样本在该属性上的值缺失，则如何划分样本？
	
3. 划分属性选择
	1. 给定训练集$\mathbb D$和属性$A$，令$\tilde{\mathbb D}$表示$\mathbb D$中在属性$A$上没有缺失的样本子集，则可以仅根据$\tilde{\mathbb D}$来判断属性$A$的优劣。
	2. 假定属性$A$有$n$个可能的取值$a_1,a_2,...,a_n$。令：
		- $\tilde{\mathbb D}^i$表示：$\tilde{\mathbb D}$中在属性$A$上取值为$a_i$的样本的子集；
		- $\tilde{\mathbb D}_k$表示：$\tilde{\mathbb D}$中属于第$k$类的样本子集（一共有$K$个分类）。
		- 根据定义：$\tilde{\mathbb D}=\cup_{k=1}^K\tilde{\mathbb D}_k=\cup_{i=1}^n\tilde{\mathbb D}^i$
	3. 为每个样本$\vec x$赋予一个权重$w_{\vec x}$，定义：
	$$
	\rho=\frac{\sum_{\vec x\in \tilde{\mathbb D}}w_{\vec x}}{\sum_{\vec x\in \mathbb D}w_{\vec x}},\tilde{p_k}=\frac{\sum_{\vec x\in \tilde{\mathbb D}_k}w_{\vec x}}{\sum_{\vec x\in \mathbb D}w_{\vec x}},\tilde{r_i}=\frac{\sum_{\vec x\in \tilde{\mathbb D}^i}w_{\vec x}}{\sum_{\vec x\in \mathbb D}w_{\vec x}} \\ k=1,2,...,K;  \ i=1,2,...,n
	$$
	​	其物理意义为：
	- $\rho$表示：无缺失值样本占总体样本的比例。
	- $\tilde{p_k}$表示：无缺失值样本中，第$k$类所占的比例。
	- $\tilde{r_i}$表示：无缺失值样本中，在属性$A$上取值为$a_i$的样本所占的比例。
	
	4. 将信息增益的计算公式推广为：
	$$
	g(\mathbb D,A)=\rho\times G(\tilde{\mathbb D},A)=\rho\times\Big(H(\tilde{\mathbb D})-\sum_{i=1}^n\tilde r_iH(\tilde{\mathbb D^i}|A)\Big)
	$$
	​		其中：$H(\tilde{\mathbb D})=-\sum^K_{k=1}\tilde p_klog\tilde p_k$
	
4. 样本划分
	1. 基于权重的样本划分：
		- 如果样本$\vec x$在划分属性$A$上的取值已知，则：将$\vec x$划入与其对应的子节点，$\vec x$的权值在子节点中保持为$w_{\vec x}$
		- 如果样本$\vec x$在划分属性$A$上的取值未知，则：将$\vec x$同时划入所有的子节点；$\vec x$的权值在所有子节点中进行调整：在属性值为$a_i$对应的子节点中，该样本的权值调整为$\tilde r_i\times w_{\vec x}$
	2. 直观地看，基于权重的样本划分就是让同一个样本以不同的概率分散到不同的子结点中去。这一做法依赖于：每个样本拥有一个权重，然后权重在子结点中重新分配。**C4.5 使用了该方案。**

## 1.12 支持向量机（SVM）
### 1.12.1 定义

#### 1.12.1.1 支持向量和支持向量机

**支持向量**：在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。在训练数据集线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量。在决定分离超平面时，只有支持向量起作用，其他的实例点并不起作用。支持向量的个数一般很少，所以支持向量机由很少的、重要的训练样本确定。

- 如果移动支持向量，将改变所求的解。
- 如果在间隔边界以外移动其他实例点，甚至去掉这些点，则解是不变的。

**间隔边界**：超平面$H_1$、$H_2$称为间隔边界,它们和分离超平面$\vec w·\vec x+b=0$平行，且没有任何实例点落在$H_1$、$H_2$之间。在$H_1$、$H_2$之间形成一条长带，分离超平面位于长带的中央。长带的宽度称为$H_1$、$H_2$之间的距离，也即间隔，间隔大小为$\frac2{||\vec w||_2}$

**支持向量机（Support Vector Machine，SVM）**：其含义是通过支持向量运算的分类器。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分离模型称为支持向量机。
在一个二维环境中，其中点R，S，G点和其它靠近中间黑线的点可以看作为支持向量，它们可以决定分类器，即黑线的具体参数。

![1.12.1](/1.12.1.png)

支持向量机是一种**二分类模型**，它的基本模型是**定义在特征空间上的间隔最大的线性分类器**。它的目的是寻找一个超平面来对样本进行分割，分割的原则是==边界最大化==，最终转化为一个==凸二次规划问题==来求解，也等价于==正则化的合页损失函数的最小化问题==。由简至繁的模型包括：
- 当训练样本线性可分时，通过硬边界（hard margin）最大化，学习一个线性可分支持向量机；
- 当训练样本近似线性可分时，通过软边界（soft margin）最大化，学习一个线性支持向量机；
- 当训练样本线性不可分时，通过核技巧和软边界最大化，学习一个非线性支持向量机；

支持向量机的最优化问题一般通过对偶问题化为凸二次规划问题求解，具体步骤是将等式约束条件带入优化目标，通过求偏导求得优化目标在不等式约束条件下的极值。

#### 1.12.1.2 核函数

**核函数**：当输入空间为欧式空间（有限维度）或离散集合，特征空间为希尔伯特空间（无穷维度）时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。

1.12.1.3 核技巧

**核技巧**：通过核函数学习非线性支持向量机等价于在高维的特征空间中学习线性支持向量机。

#### 1.12.1.4 对偶原理

**对偶原理**：一个优化问题可以从主问题和对偶问题两个方面考虑。在推导对偶问题时，通过将拉格朗日函数对x求导并使导数为0来获得对偶函数。对偶函数给出了主问题最优解的下界，因此对偶问题一般是凸问题，那么只需求解对偶问题的最优解就可以了。

#### 1.12.1.5 KKT条件

**KKT条件**：通常我们要求解的最优化条件有如下条件：

1. 无约束优化问题：通常使用求导，使导数为0，求解候选最优值；

2. 有等式约束的优化问题：通过使用拉格朗日乘子法，即把等式约束用拉格朗日乘子和优化问题合并为一个式子，通过对各个变量求导使其为0，求解候选最优值。**拉格朗日乘数法其实是KKT条件在等式约束优化问题的简化版**。

3. 有不等式约束的优化问题：通常使用KKT条件，即把不等式约束，等式约束和优化问题合并成一个式子。假设有多个等式约束$h(x)$和不等式约束$g(x)$满足
   $$
   L(x,\lambda,\mu)=f(x)+\sum_{i=1}^m\lambda_ih_i(x)+\sum_{i=1}^n\mu_jg_j(x)
   $$
   则不等式约束引入的KKT条件如下：
   $$
   \begin{cases}g_j(x)\leq0; \\ \mu_j\geq 0; \\ \mu_jg_j(x)=0\end{cases}
   $$
   实质是最优解在$g(x)<0$区域内时，约束条件不起作用，等价于对$\mu$置0然后对原函数的偏导数置零；当$g(x)=0$时域情况2相近。结合两种情况，那么只需要使$L$对$x$求导为0，使$h(x)$为0，使$\mu g(x)$为零三式即可求解候选最优值。
   
### 1.12.2 SVM中的对偶问题

**为什么要引入对偶问题**
1，对偶问题将原始问题中的约束转为了对偶问题中的等式约束，对偶问题往往更加容易求解。
2，可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的）。
3，在优化理论中，目标函数 f(x) 会有多种形式：
- 如果目标函数和约束条件都为变量 x 的线性函数，称该问题为线性规划；
- 如果目标函数为二次函数，约束条件为线性函数，称该最优化问题为二次规划；
- 如果目标函数或者约束条件均为非线性函数，称该最优化问题为非线性规划。

每个线性规划问题都有一个与之对应的对偶问题，对偶问题有非常良好的性质，以下列举几个：
​	a, 对偶问题的对偶是原问题；
​	b, 无论原始问题是否是凸的，对偶问题都是凸优化问题；
​	c, 对偶问题可以给出原始问题一个下界；
​	d, 当满足一定条件时，原始问题与对偶问题的解是完全等价的。

**如何理解SVM中的对偶问题**
在硬边界支持向量机中，问题的求解可以转化为凸二次规划问题。

​	假设优化目标为
$$
\begin{align}
&\min_{\boldsymbol w, b}\frac{1}{2}||\boldsymbol w||^2\\
&s.t. y_i(\boldsymbol w^T\boldsymbol x_i+b)\geqslant 1, i=1,2,\cdots,m.\\
\end{align}  \tag{1}
$$
**step 1**. 转化问题：
$$
\min_{\boldsymbol w, b} \max_{\alpha_i \geqslant 0}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}  \tag{2}
$$
上式等价于原问题，因为若满足(1)中不等式约束，则(2)式求max时,$\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))$必须取0，与(1)等价；若不满足(1)中不等式约束，(2)中求max会得到无穷大。 交换min和max获得其对偶问题:
$$
\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}
$$
交换之后的对偶问题和原问题并不相等，上式的解小于等于原问题的解。

**step 2**.现在的问题是如何找到问题(1) 的最优值的一个最好的下界? 
$$
\frac{1}{2}||\boldsymbol w||^2 < v\\
1 - y_i(\boldsymbol w^T\boldsymbol x_i+b) \leqslant 0\tag{3}
$$
若方程组(3)无解， 则v是问题(1)的一个下界。若(3)有解， 则 
$$
\forall \boldsymbol \alpha >  0 , \ \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} < v
$$
由逆否命题得：若 
$$
\exists \boldsymbol \alpha >  0 , \ \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} \geqslant v
$$
则(3)无解。

那么v是问题(1)的一个下界。 
 要求得一个好的下界，取最大值即可 
$$
\max_{\alpha_i \geqslant 0}  \min_{\boldsymbol w, b} \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}
$$
**step 3**. 令
$$
L(\boldsymbol w, b,\boldsymbol a) =   \frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))
$$
$p^*$为原问题的最小值，对应的$w,b$分别为$w^*,b^*$,则对于任意的$a>0$:
$$
p^* = \frac{1}{2}||\boldsymbol w^*||^2 \geqslant  L(\boldsymbol w^*, b,\boldsymbol a) \geqslant \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)
$$
则 $\min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)$是问题（1）的一个下界。

此时，取最大值即可求得好的下界，即
$$
\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)
$$

### 1.12.3 线性可分支持向量机
给定一个特征空间上的训练数据集$\mathbb D=\{(\vec x_1,\tilde y_1),(\vec x_2,\tilde y_2),...(\vec x_N,\tilde y_N)\}$，其中$\vec x_i\in \mathcal X=\mathbb R^n,\tilde y_i\in\mathcal Y=\{+1,-1\},i=1,2,...N$。$\vec x_i$为第$i$个特征向量，也称作实例；$\tilde y_i$为$\vec x_i$的类标记，$(\vec x_i,\tilde y_i)$称作样本点。
- 当$\tilde y_i=+1$时，称$\vec x_i$为正例；
- 当$\tilde y_i=-1$时，称$\vec x_i$为负例；

假设训练数据集是线性可分的，则学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程$\vec w·\vec x+b=0$，它由法向量$\vec w$和截距$b$决定，可以用$(\vec w,b)$来表示。

给定线性可分训练数据集，通过间隔最大化学习得到的分离超平面为：
$$
\vec w^*·\vec x+b^*=0
$$
相应的分类决策函数：
$$
f(\vec x)=sign(\vec w^*·\vec x+b^*)
$$
称之为**线性可分支持向量机**。

当训练数据集线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小的策略，求出分离超平面。但是此时的解有无穷多个。线性可分支持向量机**利用间隔最大化求得最优分离超平面**，这样的解只有唯一的一个。

#### 1.12.3.1 函数间隔

可以将一个点距离分离超平面的远近来表示分类预测的可靠程度：

- 一个点距离分离超平面越远，则该点的分类越可靠。
- 一个点距离分离超平面越近，则该点的分类则不那么确信。

在超平面$\vec w·\vec x+b=0$确定的情况下:

- $|\vec w·\vec x+b|$能够相对地表示点$\vec x_i$距离超平面的远近。
- $\vec w·\vec x+b$的符号与类标记$\tilde y_i$的符号是否一致能表示分类是否正确
  - $\vec w·\vec x+b>0$时，$\vec x_i$位于超平面上方，将$\vec x_i$预测为正类。此时若$\tilde y_i=+1$则分类正确；否则分类错误。
  - $\vec w·\vec x+b<0$时，$\vec x_i$位于超平面下方，将$\vec x_i$预测为负类。此时若$\tilde y_i=-1$则分类正确；否则分类错误。

**函数间隔**：用$y(\vec w·\vec x+b)$来表示分类的正确性以及确信度，符号决定了正确性，范数决定了确信度。

对于给定的训练数据集$\mathbb D$和超平面$(\vec w,b)$
- 定义超平面$(\vec w,b)$关于样本点$(\vec x_i,\tilde y_i)$的函数间隔为：$\hat \gamma_i=\tilde y_i(\vec w·\vec x_i+b)$
- 定义超平面$(\vec w,b)$关于训练集$\mathbb D$的函数间隔为：超平面$(\vec w,b)$关于$\mathbb D$中所有样本点$(\vec x_i,\tilde y_i)$的函数间隔之最小值：$\hat \gamma=min_{\mathbb D}\hat \gamma_i$

#### 1.12.3.2 几何间隔
如果成比例的改变$\vec w$和$b$，函数间隔也会在原来的基础上成比例改变。因此需要对分离超平面施加某些约束，如归一化，令$||\vec w||_2=1$，使得函数间隔是确定的。此时的函数间隔成为几何间隔。

由定义可知函数间隔和几何间隔有下列的关系：
$$
\gamma_i=\frac{\hat \gamma_i}{||\vec w||_2}, \ \gamma=\frac{\hat \gamma}{||\vec w||_2}
$$

- 当$||\vec w||_2=1$时，函数间隔和几何间隔相等。
- 当超平面参数$\vec w$,$b$等比例改变时：超平面并没有变化；函数间隔也按比例改变；几何间隔保持不变。

#### 1.12.3.3 硬间隔最大化

支持向量机学习基本思想：求解能够正确划分训练数据集并且几何间隔最大的分离超平面。**几何间隔最大化又称作硬间隔最大化**。几何间隔最大化的物理意义：不仅将正负实例点分开，而且对于最难分辨的实例点（距离超平面最近的那些点），也有足够大的确信度来将它们分开。

对于线性可分的训练数据集而言，线性可分分离超平面有无穷多个（等价于感知机），但**几何间隔最大的分离超平面是唯一的**。

求解几何间隔最大的分离超平面可以表示为约束的最优化问题：
$$
\underset{\vec w,b}{max}\gamma \\ s.t. \  \tilde y_i(\frac{\vec w}{||\vec w||_2}·\vec x_i+\frac b{||\vec w||_2})\geq \gamma,i=1,2,...,N
$$
考虑几何间隔和函数间隔的关系，改写问题为：
$$
\underset{\vec w,b}{max}\frac{\hat\gamma}{||\vec w||_2} \\ s.t. \  \tilde y_i(\vec w·\vec x_i+b)\geq \gamma,i=1,2,...,N
$$
函数间隔$\hat\gamma$的大小并不影响最优化问题的解。因此取$\hat\gamma=1$，则最优化问题改写为：
$$
\underset{\vec w,b}{max}\frac{1}{||\vec w||_2} \\ s.t. \  \tilde y_i(\vec w·\vec x_i+b)\geq 1,i=1,2,...,N
$$
注意到$max\frac{1}{||\vec w||_2}$和$min\frac12||\vec w||_2^2$是等价的，于是最优化问题改写为：
$$
\underset{\vec w,b}{min}\frac12{||\vec w||_2^2} \\ s.t. \  \tilde y_i(\vec w·\vec x_i+b)-1\geq 0,i=1,2,...,N
$$
这是一个凸二次规划问题。

凸优化问题 ，指约束最优化问题：
$$
\underset{\vec w}{min}f(\vec w) \\ s.t. \ g_j(\vec w)\leq0,j=1,2,...,J \\ h_k(\vec w)=0,k=1,2,...,K
$$
其中：目标函数$f(\vec w)$和约束函数$g_j(\vec w)$都是$\mathbb R^n$上的连续可微的凸函数。约束函数$h_k(\vec w)$是$\mathbb R^n$上的仿射函数，满足$h(\vec x)=\vec a·\vec x+b,\vec a\in\mathbb R^n,x\in\mathbb R^n$。

当目标函数$f(\vec w)$是二次函数且约束函数$g_j(\vec w)$是仿射函数时，上述凸最优化问题成为凸二次规划问题。

#### 1.12.3.4 支持向量

在线性不可分的情况下，对偶问题的解$\vec \alpha^*=(\alpha_1^*,\alpha_2^*,...\alpha_N^*) ^T$ 中，对应于$\alpha_i^*>0$ 的样本点 $(\vec x_i,\tilde y_i )$的实例点 称作支持向量，它是软间隔的支持向量。

线性不可分的支持向量比线性可分时的情况复杂一些：根据 $\nabla_{\xi_i}L(\vec w,b,\vec \xi,\vec \alpha,\vec \mu)=C-\alpha_i-\mu_i=0 $，以及$\mu_j^*\xi_j^*=0$ ，则：

- 若 $\alpha_i^*<C$，则 $\mu_i>0$， 则松弛量 $\xi_i=0$。此时：支持向量恰好落在了间隔边界上。
- 若$\alpha_i^*=C$ ， 则$\mu_i=0$ ，于是 $\xi_i$ 可能为任何正数：
  - 若$0<\xi_i<1$ ，则支持向量落在间隔边界与分离超平面之间，分类正确。
  - 若 $\xi_i=1$，则支持向量落在分离超平面上。
  - 若 $\xi_i>1$，则支持向量落在分离超平面误分类一侧，分类错误。

#### 1.12.3.5 合页损失函数

定义取正函数为：
$$
plus(z)=\begin{cases} z,z>0 \\ 0, z\leq 0 \end{cases}
$$
定义合页损失函数为：$L(\tilde y,\hat y)=plus(1-\tilde y\hat y)$ ，其中$\tilde y$ 为样本的标签值， $\hat y$为样本的模型预测值。则线性支持向量机就是最小化目标函数：
$$
\sum_{i=1}^Nplus(1-\tilde y_i(\vec w·\vec x_i+b))+\lambda||\vec w||_2^2, \lambda>0
$$
合页损失函数的物理意义：

- 当样本点$(\vec x_i,\tilde y_i )$ 被正确分类且函数间隔（确信度）$\tilde y_i(\vec w·\vec x_i+b)$ 大于 1 时，损失为0
- 当样本点$(\vec x_i,\tilde y_i )$ 被正确分类且函数间隔（确信度） $\tilde y_i(\vec w·\vec x_i+b)$小于等于 1 时损失为$1-\tilde y_i(\vec w·\vec x_i+b)$
- 当样本点 $(\vec x_i,\tilde y_i )$未被正确分类时损失为$1-\tilde y_i(\vec w·\vec x_i+b)$

合页损失函数图形如下：

- 合页损失函数不仅要分类正确，而且要确信度足够高（确信度为1）时，损失才是0。即合页损失函数对学习有更高的要求。

- 0-1损失函数通常是二分类问题的真正的损失函数，合页损失函数是0-1损失函数的上界。

- 通常都是用0-1损失函数的上界函数构成目标函数，这时的上界损失函数又称为代理损失函数。

  ![1.12.3.5](/1.12.3.5.png)

#### 1.12.3.6 线性可分支持向量机原始算法

- 输入：线性可分训练数据集$\mathbb D=\{(\vec x_1,\tilde y_1),(\vec x_2,\tilde y_2),...(\vec x_N,\tilde y_N)\}$，其中$\vec x_i\in \mathcal X=\mathbb R^n,\tilde y_i\in\mathcal Y=\{+1,-1\},i=1,2,...N$。

- 输出：最大几何间隔的分离超平面；分类决策函数

- 算法步骤：构造并且求解约束最优化问题：
  $$
  \underset{\vec w,b}{min}\frac12{||\vec w||_2^2} \\ s.t. \  \tilde y_i(\vec w·\vec x_i+b)-1\geq 0,i=1,2,...,N
  $$
  求得最优解$\vec w^*,b^*$,由此得到分离超平面：$\vec w^*·\vec x+b^*=0$，以及分类决策函数 ：$f(\vec x)=sign(\vec w^*·\vec x+b^*)$

#### 1.12.3.7 线性可分支持向量机对偶算法

将线性可分支持向量机的最优化问题作为原始最优化问题，应用**拉格朗日对偶性**，通过求解对偶问题得到原始问题的最优解。这就是线性可分支持向量机的对偶算法。

对偶算法的优点：对偶问题往往更容易求解；引入了核函数，进而推广到非线性分类问题。

原始问题：
$$
\underset{\vec w,b}{min}\frac12{||\vec w||_2^2} \\ s.t. \  \tilde y_i(\vec w·\vec x_i+b)-1\geq 0,i=1,2,...,N
$$
定义拉格朗日函数：
$$
L(\vec w,b,\vec \alpha)=\frac12||\vec w||_2^2-\sum_{i=1}^N\alpha_i\tilde y_i(\vec w·\vec x_i+b)+\sum_{i=1}^N\alpha_i
$$
其中$\vec \alpha=(\alpha_1,\alpha_2,...\alpha_N)^T$为拉格朗日乘子向量。

- 根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
  $$
  \underset{\vec \alpha}{max} \ \underset{\vec w,b}{min}L(\vec w,b,\vec\alpha)
  $$
  先求$\underset{\vec w,b}{min}L(\vec w,b,\vec\alpha)$。拉格朗日函数分别为$\vec w,b$求偏导数，并令其等于 0
  $$
  \nabla_{\vec w}L(\vec w,b,\vec\alpha)=\vec w-\sum_{i=1}^N\alpha_i\tilde y_i\vec x_i=0 \\ \nabla_bL(\vec w,b,\vec\alpha)=\sum_{i=1}^N\alpha_i\tilde y_i=0 \\ \Rightarrow\vec w=\sum_{i=1}^N\alpha_i\tilde y_i\vec x_i=0, \ \sum_{i=1}^N\alpha_i\tilde y_i=0
  $$
  代入拉格朗日函数：
  $$
  L(\vec w,b,\vec\alpha)=\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j\tilde y_i\tilde y_j(\vec x_i,\vec x_j)-\sum_{i=1}^N\alpha_i\tilde y_i[(\sum_{j=1}^N\alpha_j\tilde y_j\vec x_j)·\vec x_i+b]+\sum_{i=1}^N\alpha_i \\ =-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j\tilde y_i\tilde y_j(\vec x_i,\vec x_j)+\sum_{i=1}^N\alpha_i
  $$
  对偶问题极大值为：
  $$
  \underset{\vec\alpha}{max}-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j\tilde y_i\tilde y_j(\vec x_i,\vec x_j)+\sum_{i=1}^N\alpha_i \\ s.t. \ \sum_{i=1}^N\alpha_i\tilde y_i=0 \\ \alpha_i\geq0,i=1,2,...N
  $$
  设对偶最优化问题的$\vec\alpha$的解为$\vec\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_N^*)$，则根据 KKT 条件有：
  $$
  \nabla_{\vec w}L(\vec w^*,b^*,\vec\alpha^*)=\vec w^*-\sum_{i=1}^N\alpha_i^*\tilde y_i\vec x_i=0 \\ \nabla_bL(\vec w^*,b^*,\vec\alpha^*)=\sum_{i=1}^N\alpha_i^*\tilde y_i=0 \\ \alpha_i^*[\tilde y_i(\vec w^*·\vec x_i+b^*)-1]=0,i=1,2,...N \\ \tilde y_i(\vec w^*·\vec x_i+b^*)-1\geq0,i=1,2,...N \\ \alpha_i^*\geq0,i=1,2,...N
  $$
  根据第一个式子，有：
  $$
  \vec w^*=\sum_{i=1}^N\alpha_i^*\tilde y_i\vec x_i
  $$
  由于$\vec\alpha^*$不可能是零向量，则必然存在某个$j$使得$\alpha_j^*>0$根据第三个式子，此时必有$\tilde y_i(\vec w^*·\vec x_i+b^*)-1=0$。同时考虑到$\tilde y_j^2=1$，得到 ：
  $$
  b^*=\tilde y_j-\sum_{i=1}^N\alpha_i^*\tilde y_i(\vec x_i,\vec x_j)
  $$
  于是分离超平面写作：
  $$
  \sum_{i=1}^N\vec\alpha_i^*\tilde y_i(\vec x·\vec x_i)+b^*=0
  $$
  分类决策函数写作：
  $$
  f(\vec x)=sign(\sum_{i=1}^N\alpha_i^*\tilde y_i(\vec x·\vec x_i)+b^*)
  $$
  上式称作线性可分支持向量机的对偶形式。可以看到：分类决策函数只依赖于输入$\vec x$和训练样本的内积。

$\vec w,b$只依赖于$\alpha_i^*>0$对应的样本点$\vec x_i,\tilde y_i$，而其他的样本点对于$\vec w,b$没有影响。

将训练数据集里面对应于$\alpha_i^*>0$的样本点对应的实例$\vec x_i$称为支持向量。



**线性可分支持向量机对偶算法**：

- 输入：线性可分训练数据集$\mathbb D=\{(\vec x_1,\tilde y_1),(\vec x_2,\tilde y_2),...(\vec x_N,\tilde y_N)\}$，其中$\vec x_i\in \mathcal X=\mathbb R^n,\tilde y_i\in\mathcal Y=\{+1,-1\},i=1,2,...N$。

- 输出：最大几何间隔的分离超平面；分类决策函数

- 算法步骤：构造并且求解约束最优化问题：
  $$
  \underset{\vec \alpha}{min}\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j\tilde y_i\tilde y_j(\vec x_i·\vec x_j)-\sum_{i=1}^N\alpha_i \\ s.t. \ \sum_{i=1}^N\alpha_i\tilde y_i=0, \ \alpha_i\geq0,i=1,2,...N
  $$
  求得最优解$\vec \alpha^*=(\alpha_1^*,\alpha_2^*,...\alpha_N^*)^T$。

  计算$\vec w^*=\sum_{i=1}^N\alpha_i^*\tilde y_i\vec x_i$.

  选择$\vec \alpha^*$的一个正的分量$\alpha_j^*>0$，计算$b^*=\tilde y_j-\sum_{i=1}^N\alpha_i^*\tilde y_i(\vec x_i,\vec x_j)$

  由此得到分离超平面：$\vec w^*·\vec x+b^*=0$，以及分类决策函数 ：$f(\vec x)=sign(\vec w^*·\vec x+b^*)$

#### 1.12.3.8 线性支持向量机

对于线性不可分训练数据，线性支持向量机不再适用，但可以想办法将它扩展到线性不可分问题。

原始问题：

假设训练数据集不是线性可分的，这意味着某些样本点$(\vec x_i,\tilde y_i)$不满足函数间隔大于等于 1 的约束条件。对每个样本点引进一个松弛变量$\xi_i\geq0$，使得函数间隔加上松弛变量大于等于 1。即约束条件变成了：$\tilde y_i(\vec w·\vec x_i)\geq1-\xi_i$。

对每个松弛变量$\xi_i$，支付一个代价$\xi_i$，目标函数变为：
$$
min\frac12||\vec w||_2^2+C\sum_{i=1}^N\xi_i
$$
这里$C>0$称作惩罚参数，一般由应用问题决定。

- C值大时，对误分类的惩罚增大，此时误分类点凸显的更重要
- C值较大时，对误分类的惩罚增加，此时误分类点比较重要。
- 值较小时，对误分类的惩罚减小，此时误分类点相对不重要。

相对于硬间隔最大化，$\frac12||\vec w||_2^2+C\sum_{i=1}^N\xi_i$称为**软间隔最大化**。于是线性不可分的线性支持向量机的学习问题变成了凸二次规划问题：
$$
\underset{\vec w,b,\vec\xi}{min}\frac12||\vec w||_2^2+C\sum_{i=1}^N\xi_i \\ s.t. \ \tilde y_i(\vec w·\vec x_i+b)\geq1-\xi_i,i=1,2,...N \\ \xi_i\geq0,i=1,2,...N
$$
可以证明$\vec w$的解是唯一的；b 的解不是唯一的，b 的解存在于一个区间。

**线性支持向量机**：对于给定的线性不可分的训练集数据，通过求解软间隔最大化问题得到的分离超平面为：$\vec w^*·\vec x+b^*=0$以及相应的分类决策函数：$f(\vec x)=\vec w^*·\vec x+b^*$，称之为线性支持向量机。线性支持向量机包含线性可分支持向量机，且具有更广泛的适用性。

**对偶问题**：

原始问题是拉格朗日函数的极小极大问题；对偶问题是拉格朗日函数的极大极小问题。
$$
L(\vec w,b,\vec\xi,\vec\alpha,\vec\mu)=\frac12||\vec w||_2^2+C\sum^N_{i=1}\xi_i-\sum_{i=1}^N\alpha_i[\tilde y_i(\vec w_i·\vec x_i+b)-1+\xi_i]-\sum_{i=1}^N\mu_i\xi_i \\ \alpha_i\geq0,\mu_i\geq0
$$

- 先求$L(\vec w,b,\vec\xi,\vec\alpha,\vec\mu)$对$\vec w,b,\vec\xi$的的极小。根据偏导数为0：
  $$
  \nabla_{\vec w}L(\vec w,b,\vec\xi,\vec\alpha,\vec\mu)=\vec w-\sum_{i=1}^N\alpha_i\tilde y_i\vec x_i=\vec 0 \\ \nabla_bL(\vec w,b,\vec\xi,\vec\alpha,\vec\mu)=-\sum_{i=1}^N\alpha_i\tilde y_i=0 \\ \nabla_{\xi_i}L(\vec w,b,\vec\xi,\vec\alpha,\vec\mu)=C-\alpha_i-\mu_i=0
  $$
  得到：
  $$
  \vec w=\sum_{i=1}^N\alpha_i\tilde y_i\vec x_i \\ \sum_{i=1}^N\alpha_i\tilde y_i=0 \\ C-\alpha_i-\mu_i=0
  $$
  
- 再求极大问题：将上面三个等式代入拉格朗日函数：
  $$
  \underset{\vec\alpha,\vec\mu}{max}\underset{\vec w,b,\vec \xi}{min}L(\vec w,b,\vec\xi,\vec\alpha,\vec\mu)=\underset{\vec\alpha,\vec\mu}{max}[-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j\tilde y_i\tilde y_j(\vec x_i·\vec x_j)+\sum_{i=1}^N\alpha_i]
  $$
  于是得到对偶问题：
  $$
  \underset{\vec\alpha}{min}\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j\tilde y_i\tilde y_j(\vec x_i·\vec x_j)-\sum_{i=1}^N\alpha_i \\ s.t. \sum_{i=1}^N\alpha_i\tilde y_i=0 \\ 0 \leq\alpha_i\leq C,i=1,2,...N
  $$

根据 KKT 条件可得到：$\vec w^*=\sum_{i=1}^N\vec\alpha^*\tilde y_i\vec x_i$

- 若存在$\vec\alpha^*$的某个分量$\alpha_j^*$，$0<\alpha_j^*<C$，则有$\mu_j^*=C-\alpha_j^*>0$

  若$\vec\alpha^*$的所有分量都等于 0 ，则得出$\vec w^*$为零，没有任何意义。
  若$\vec\alpha^*$的所有分量都等于$C$，根据$\sum \alpha_i^*\tilde y_i=0$，则要求$\sum \tilde y_i=0$。这属于强加的约束

  - 根据$\mu_j^*\xi_j^*=0$，有$\xi_j^*=0$ 
  - 考虑 $\alpha_j^*[\tilde y_j(\vec w^*·\vec x_j+b^*)-1+\xi_j^*]=0$，则有：$b^*=\tilde y_j-\sum_{i=1}^N\vec\alpha^*\tilde y_i(\vec x_i·\vec x_j)$

- 分离超平面为：$\sum_{i=1}^N\vec\alpha^*\tilde y_i(\vec x_i·\vec x)+b^*=0$

- 分类决策函数为：$f(\vec x)=sign[\sum_{i=1}^N\vec\alpha^*\tilde y_i(\vec x_i·\vec x)+b^*]$

线性支持向量机对偶算法：

- 输入：训练数据集$\mathbb D=\{(\vec x_1,\tilde y_1),(\vec x_2,\tilde y_2),...(\vec x_N,\tilde y_N)\}$，其中$\vec x_i\in \mathcal X=\mathbb R^n,\tilde y_i\in\mathcal Y=\{+1,-1\},i=1,2,...N$。

- 输出：分离超平面；分类决策函数

- 算法步骤：

  - 选择惩罚参数$C>0$ ，构造并且求解约束最优化问题：
    $$
    \underset{\vec\alpha}{min}\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j\tilde y_i\tilde y_j(\vec x_i·\vec x_j)-\sum_{i=1}^N\alpha_i \\ s.t. \sum_{i=1}^N\alpha_i\tilde y_i=0 \\ 0 \leq\alpha_i\leq C,i=1,2,...N
    $$
    求得最优解$\vec\alpha^*=(\alpha_1^*,\alpha_2^*,...\alpha_N^*)^T$

  - 计算 ：$\vec w^*=\sum_{i=1}^N\vec\alpha^*\tilde y_i\vec x_i$

  - 选择$\vec \alpha^*$ 的一个合适的分量 $0< \alpha_j^* <C$，计算：$b^*=\tilde y_j-\sum^N_{i=1}\alpha_i^*\tilde y_i(\vec x_i·\vec x_j)$ 。

    可能存在多个符合条件的$\alpha_j^*$ 。这是由于原始问题中，对 b 的解不唯一。所以实际计算时可以取在所有符合条件的样本点上的平均值。

  - 由此得到分离超平面：$\vec w^*·\vec x+b^*=0$ ，以及分类决策函数：$f(\vec x)=sign(\vec w^*·\vec x+b^*)$

#### 1.12.3.9 非线性支持向量机

非线性分类问题是指利用非线性模型才能很好的进行分类的问题。

设原空间为 $ \mathcal X \subset\mathbb R,\vec x=(x_1,x_2)^T\in\mathcal X$，新的空间为$\mathcal Z\subset\mathbb R^2,\vec z=(z_1,z_2)^T\in\mathcal Z$ 。定义从原空间到新空间的变换（映射）为：$\vec z=  \phi (\vec x)=(x_1^2,x^2_2)^T $ 。则经过变换:

- 原空间 $\mathcal X\subset\mathbb R^2$变换为新空间 $\mathcal Z \subset \mathbb R^2$， 原空间中的点相应地变换为新空间中的点。
- 原空间中的椭圆$w_1x_1^2+w_2x_2^2+b=0$ 变换为新空间中的直线$w_1z_1+w_2z_2+b=0$
- 若在变换后的新空间，直线 $w_1z_1+w_2z_2+b=0$可以将变换后的正负实例点正确分开，则原空间的非线性可分问题就变成了新空间的线性可分问题。

### 支持向量机能解决哪些问题

**线性分类**

- 在训练数据中，每个数据都有n个的属性和一个二分类类别标志，我们可以认为这些数据在一个n维空间里。我们的目标是找到一个n-1维的超平面，这个超平面可以将数据分成两部分，每部分数据都属于同一个类别。
- 这样的超平面有很多，假如我们要找到一个最佳的超平面。此时，**增加一个约束条件：要求这个超平面到每边最近数据点的距离是最大的，成为最大边距超平面。这个分类器即为最大边距分类器**。

**非线性分类**
- SVM的一个优势是**支持非线性分类**。它结合使用==拉格朗日乘子法（Lagrange Multiplier）和KKT（Karush Kuhn Tucker）条件，以及核函数==可以生成非线性分类器。

## 引用

### **如何画ROC曲线**
下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。

![1.4.10.1](/1.4.10.1.jpg)

步骤：
	1、假设已经得出一系列样本被划分为正类的概率，按照大小排序。
	2、从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。
	3、每次选取一个不同的threshold，得到一组FPR和TPR，即ROC曲线上的一点。以此共得到20组FPR和TPR的值。
	4、根据3、中的每个坐标点，画图。

### 计算TPR，FPR
1、分析数据
y_true = [0, 0, 1, 1]；scores = [0.1, 0.4, 0.35, 0.8]；
2、列表

| 样本 | 预测属于P的概率(score) | 真实类别 |
| ---- | ---------------------- | -------- |
| y[0] | 0.1                    | N        |
| y[1] | 0.4                    | N        |
| y[2] | 0.35                   | P        |
| y[3] | 0.8                    | P        |

3、将截断点依次取为score值，计算TPR和FPR。
当截断点为0.1时：
说明只要score>=0.1，它的预测类别就是正例。 因为4个样本的score都大于等于0.1，所以，所有样本的预测类别都为P。
scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [1, 1, 1, 1]；
正例与反例信息如下：

|          | 正例 | 反例 |
| -------- | ---- | ---- |
| **正例** | TP=2 | FN=0 |
| **反例** | FP=2 | TN=0 |

由此可得：
TPR = TP/(TP+FN) = 1； FPR = FP/(TN+FP) = 1；

当截断点为0.35时：
scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 1, 1, 1];
正例与反例信息如下：

|          | 正例 | 反例 |
| -------- | ---- | ---- |
| **正例** | TP=2 | FN=0 |
| **反例** | FP=1 | TN=1 |

由此可得：
TPR = TP/(TP+FN) = 1； FPR = FP/(TN+FP) = 0.5；

当截断点为0.4时：
scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 1, 0, 1]；
正例与反例信息如下：

|          | 正例 | 反例 |
| -------- | ---- | ---- |
| **正例** | TP=1 | FN=1 |
| **反例** | FP=1 | TN=1 |

由此可得：
TPR = TP/(TP+FN) = 0.5； FPR = FP/(TN+FP) = 0.5；

当截断点为0.8时：
scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 0, 0, 1]；

正例与反例信息如下：

|          | 正例 | 反例 |
| -------- | ---- | ---- |
| **正例** | TP=1 | FN=1 |
| **反例** | FP=0 | TN=2 |

由此可得：
TPR = TP/(TP+FN) = 0.5； FPR = FP/(TN+FP) = 0；

4、根据TPR、FPR值，以FPR为横轴，TPR为纵轴画图。

### AUC计算画图

现在假设有一个训练好的二分类器对10个正负样本（正例5个，负例5个）预测，得分按高到低排序得到的最好预测结果为[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]，即5个正例均排在5个负例前面，正例排在负例前面的概率为100%。然后绘制其ROC曲线，由于是10个样本，除去原点我们需要描10个点，如下：

![](/1.4.11.2.png)

​	描点方式按照样本预测结果的得分高低从左至右开始遍历。从原点开始，每遇到1便向y轴正方向移动y轴最小步长1个单位，这里是1/5=0.2；每遇到0则向x轴正方向移动x轴最小步长1个单位，这里也是0.2。不难看出，上图的AUC等于1，印证了正例排在负例前面的概率的确为100%。

​	假设预测结果序列为[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]。

![](/1.4.11.3.png)

​	计算上图的AUC为0.96与计算正例与排在负例前面的概率0.8 × 1 + 0.2 × 0.8 = 0.96相等，而左上角阴影部分的面积则是负例排在正例前面的概率0.2 × 0.2 = 0.04。

​	假设预测结果序列为[1, 1, 1, 0, 1, 0, 1, 0, 0, 0]。

![](/1.4.11.4.png)

​	计算上图的AUC为0.88与计算正例与排在负例前面的概率0.6 × 1 + 0.2 × 0.8 + 0.2 × 0.6 = 0.88相等，左上角阴影部分的面积是负例排在正例前面的概率0.2 × 0.2 × 3 = 0.12。

### 样本标准差为什么除以n-1
假设总体的均值为$\mu$，总体的方差为$\sigma^2$，样本为随机向量$(x_1,x_2,...x_n)$，样本的均值为$\overline x$，样本的方差为$s^2$。从公式上而言，样本方差的估计值的期望要等于总体方差，如下
$$
E(s^2)=\sigma^2
$$
 但是没有修正的方差公式，它的期望是不等于总体方差的 
$$
E(s^2)=E(\frac1n\sum^n_{i=1}(x_i-\overline x)^2)\neq \sigma^2
$$
 也就是说，样本方差估计量如果是用没有修正的方差公式来估计总计方差的话是有偏差的 .
 下面给出比较好理解的公式推导过程： 
$$
s^2=\frac1n\sum_{i=1}^n(x_i-\overline x)^2=\frac1n\sum_{i=1}^n((x_i-\mu)+(\mu-\overline x))^2 \\ =\frac1n\sum_{i=1}^n(x_i-\mu)^2-\frac2n\sum_{i=1}^n(x_i-\mu)(\mu-\overline x)+\frac1n\sum_{i=1}^n(\mu-\overline x)^2 \\ =\frac1n\sum_{i=1}^n(x_i-\mu)^2-2(\overline x-\mu)(\mu-\overline x)+(\mu-\overline x)^2 \\ =\frac1n\sum_{i=1}^n(x_i-\mu)^2-(\mu-\overline x)^2\leq\frac1n\sum_{i=1}^n(x_i-\mu)^2
$$
 需要注意的是不等式右边的才是的对方差的“正确”估计，但是我们是不知道真正的总体均值是多少的，只能通过样本的均值来代替总体的均值。所以样本方差估计量如果是用没有修正的方差公式来估计总计方差的话是会有偏差，是会低估了总体的样本方差的。为了能无偏差的估计总体方差，所以要对方差计算公式进行修正，修正公式如下： 
$$
s^2=\frac{n}{n-1}(\frac1n\sum^n_{i=1}(x_i-\overline x)^2)
$$
 这种修正后的估计量将是总体方差的无偏估计量，下面将会给出这种修正的一个来源:
1.  方差计算公式：   
$$
D(x)=E[x^2]-(E[x])^2
$$
2. 均值的均值、方差计算公式：
$$
E[\overline x]=\overline x, D[\overline x]=D[\frac1n\sum_{i=1}^nx_i]=\frac1nD[x_i]=\frac1nD[x](i=1,2,...n)
$$
3. 对于没有修正的方差计算公式我们有： 

$$
E(s^2)=E(\frac1n\sum_{i=1}^n(x_i-\overline x)^2) \\ =E[\frac1n\sum_{i=1}^n(x_i)^2=\frac2n\sum_{i=1}^n(x_i)(\overline x)+\frac1n\sum_{i=1}^n(\overline x)^2] \\ =E[\frac1n\sum_{i=1}^n(x_i)^2-2(\overline x)^2+(\overline x)^2] \\ =E[\frac1n\sum_{i=1}^n(x_i)^2-(\overline x)^2] \\ =E[\frac1n\sum_{i=1}^n(x_i)^2]-E[(\overline x)^2] \\ = E[(x_i)^2]-E[(\overline x)^2] \\ =D[(x_i)^2]+(E[x_i])^2-(D[(\overline x)^2]+(E[\overline x])^2)
$$

 因为：
$$
E[\overline x]=\overline x=E[x_i], D[\overline x]=\frac1nD[x](i=1,2,...n)
$$
所以有：
$$
E[s^2]=D(x)-\frac1nD(x)=\frac{n-1}nD(x)=\frac{n-1}n\sigma^2
$$
 在这里如果想修正的方差公式，让修正后的方差公式求出的方差的期望为总体方差的话就需要在没有修正的方差公式前面加上来进行修正，即： 
$$
\frac n{n-1}E(s^2)=\frac n{n-1}\times\frac{n-1}nD(x)=\frac n{n-1}\times\frac{n-1}n\sigma^2=\sigma^2
$$
 所以就会有这样的修正公式： 
$$
s^2=\frac n{n-1}(\frac1n\sum^n_{i=1}(x_i-\overline x)^2) \\ =\frac1{n-1}\sum^n_{i=1}(x_i-\overline x)^2)
$$
 这就解释了为什么要对方差计算公式进行修正，且为什么要这样修正。 